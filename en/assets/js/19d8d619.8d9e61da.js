"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6803],{44390:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>c,frontMatter:()=>d,metadata:()=>l,toc:()=>a});var s=i(85893),t=i(11151);const d={sidebar_position:1},r="\u4eba\u8138\u68c0\u6d4b",l={id:"CanaanK230/part14/part2/faceDetection",title:"\u4eba\u8138\u68c0\u6d4b",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part2/01-faceDetection.md",sourceDirName:"CanaanK230/part14/part2",slug:"/CanaanK230/part14/part2/faceDetection",permalink:"/en/CanaanK230/part14/part2/faceDetection",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part2/01-faceDetection.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"canaanK230Sidebar",previous:{title:"\u4eba\u8138\u76f8\u5173",permalink:"/en/category/\u4eba\u8138\u76f8\u5173"},next:{title:"\u4eba\u8138\u5173\u952e\u90e8\u4f4d",permalink:"/en/CanaanK230/part14/part2/faceLandmark"}},o={},a=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u6838\u5fc3\u4ee3\u7801",id:"2\u6838\u5fc3\u4ee3\u7801",level:2},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"\u4eba\u8138\u68c0\u6d4b",children:"\u4eba\u8138\u68c0\u6d4b"}),"\n",(0,s.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,s.jsx)(n.p,{children:"\u5b66\u4e60\u6444\u50cf\u5934\u8fdb\u884c\u4eba\u8138\u68c0\u6d4b\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"2\u6838\u5fc3\u4ee3\u7801",children:"2.\u6838\u5fc3\u4ee3\u7801"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u7c7b\u5b9a\u4e49\u4e0e\u521d\u59cb\u5316"}),"\uff1a","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"FaceDetectionApp"}),"\u7ee7\u627f\u81ea",(0,s.jsx)(n.code,{children:"AIBase"}),"\uff0c\u8d1f\u8d23\u6a21\u578b\u52a0\u8f7d\u548c\u57fa\u7840\u63a8\u7406\u529f\u80fd\u3002"]}),"\n",(0,s.jsx)(n.li,{children:"\u521d\u59cb\u5316\u53c2\u6570\u5305\u62ec\u6a21\u578b\u8def\u5f84\u3001\u8f93\u5165\u5c3a\u5bf8\u3001\u951a\u70b9\u6570\u636e\u3001\u7f6e\u4fe1\u5ea6\u9608\u503c\u3001NMS\u9608\u503c\u7b49\u3002"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Ai2d"}),"\u5b9e\u4f8b\u7528\u4e8e\u56fe\u50cf\u9884\u5904\u7406\uff08\u586b\u5145\u548c\u7f29\u653e\uff09\uff0c\u914d\u7f6e\u4e3aNCHW\u683c\u5f0f\u7684uint8\u7c7b\u578b\u6570\u636e\u3002"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u9884\u5904\u7406\u914d\u7f6e"}),"\uff1a","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"config_preprocess"}),"\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u586b\u5145\u53c2\u6570\uff0c\u4fdd\u6301\u56fe\u50cf\u5bbd\u9ad8\u6bd4\uff0c\u907f\u514d\u53d8\u5f62\u3002\u4f7f\u7528",(0,s.jsx)(n.code,{children:"pad"}),"\u586b\u5145\u9ed1\u8fb9\uff0c",(0,s.jsx)(n.code,{children:"resize"}),"\u5c06\u56fe\u50cf\u7f29\u653e\u5230\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u3002"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"get_padding_param"}),"\u8ba1\u7b97\u586b\u5145\u7684\u4e0a\u4e0b\u5de6\u53f3\u8fb9\u8ddd\uff0c\u786e\u4fdd\u56fe\u50cf\u5c45\u4e2d\u7f29\u653e\u3002"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u6a21\u578b\u63a8\u7406\u4e0e\u540e\u5904\u7406"}),"\uff1a","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"postprocess"}),"\u8c03\u7528",(0,s.jsx)(n.code,{children:"aidemo.face_det_post_process"}),"\u5904\u7406\u6a21\u578b\u8f93\u51fa\uff0c\u5e94\u7528\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548cNMS\uff0c\u8fd4\u56de\u68c0\u6d4b\u7ed3\u679c\u3002"]}),"\n",(0,s.jsx)(n.li,{children:"\u540e\u5904\u7406\u5c06\u6a21\u578b\u8f93\u51fa\u7684\u76f8\u5bf9\u5750\u6807\u8f6c\u6362\u4e3a\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8\uff0c\u8003\u8651\u9884\u5904\u7406\u4e2d\u7684\u586b\u5145\u548c\u7f29\u653e\u3002"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u7ed3\u679c\u7ed8\u5236"}),"\uff1a","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"draw_result"}),"\u5c06\u68c0\u6d4b\u6846\u5750\u6807\u4ece\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8\uff08",(0,s.jsx)(n.code,{children:"rgb888p_size"}),"\uff09\u6620\u5c04\u5230\u663e\u793a\u5c3a\u5bf8\uff08",(0,s.jsx)(n.code,{children:"display_size"}),"\uff09\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u8bbe\u5907\u6b63\u786e\u663e\u793a\u3002"]}),"\n",(0,s.jsxs)(n.li,{children:["\u4f7f\u7528",(0,s.jsx)(n.code,{children:"PipeLine"}),"\u7684",(0,s.jsx)(n.code,{children:"osd_img"}),"\u5728\u5c4f\u5e55\u4e0a\u7ed8\u5236\u77e9\u5f62\u6846\uff0c\u989c\u8272\u4e3a\u9752\u8272\uff0c\u7ebf\u5bbd2\u50cf\u7d20\u3002"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u4e3b\u6d41\u7a0b"}),"\uff1a","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u521d\u59cb\u5316\u663e\u793a\u6a21\u5f0f\u548c\u56fe\u50cf\u5c3a\u5bf8\uff0c\u52a0\u8f7d\u6a21\u578b\u53ca\u951a\u70b9\u6570\u636e\u3002"}),"\n",(0,s.jsxs)(n.li,{children:["\u521b\u5efa",(0,s.jsx)(n.code,{children:"PipeLine"}),"\u5b9e\u4f8b\u5904\u7406\u56fe\u50cf\u6d41\uff0c\u5faa\u73af\u83b7\u53d6\u5e27\u6570\u636e\uff0c\u6267\u884c\u63a8\u7406\u548c\u7ed8\u5236\u3002"]}),"\n",(0,s.jsx)(n.li,{children:"\u5f02\u5e38\u5904\u7406\u548c\u8d44\u6e90\u91ca\u653e\u786e\u4fdd\u7a0b\u5e8f\u7a33\u5b9a\u9000\u51fa\u3002"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass FaceDetectionApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, anchors, confidence_threshold=0.5, nms_threshold=0.2, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.model_input_size = model_input_size  # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.confidence_threshold = confidence_threshold  # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.nms_threshold = nms_threshold  # NMS\uff08\u975e\u6781\u5927\u503c\u6291\u5236\uff09\u9608\u503c\n        self.anchors = anchors  # \u951a\u70b9\u6570\u636e\uff0c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]  # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]  # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.ai2d = Ai2d(debug_mode)  # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):  # \u8ba1\u65f6\u5668\uff0c\u5982\u679cdebug_mode\u5927\u4e8e0\u5219\u5f00\u542f\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            top, bottom, left, right = self.get_padding_param()  # \u83b7\u53d6padding\u53c2\u6570\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [104, 117, 123])  # \u586b\u5145\u8fb9\u7f18\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)  # \u7f29\u653e\u56fe\u50cf\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            post_ret = aidemo.face_det_post_process(self.confidence_threshold, self.nms_threshold, self.model_input_size[1], self.anchors, self.rgb888p_size, results)\n            if len(post_ret) == 0:\n                return post_ret\n            else:\n                return post_ret[0]\n\n    # \u7ed8\u5236\u68c0\u6d4b\u7ed3\u679c\u5230\u753b\u9762\u4e0a\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            if dets:\n                pl.osd_img.clear()  # \u6e05\u9664OSD\u56fe\u50cf\n                for det in dets:\n                    # \u5c06\u68c0\u6d4b\u6846\u7684\u5750\u6807\u8f6c\u6362\u4e3a\u663e\u793a\u5206\u8fa8\u7387\u4e0b\u7684\u5750\u6807\n                    x, y, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                    x = x * self.display_size[0] // self.rgb888p_size[0]\n                    y = y * self.display_size[1] // self.rgb888p_size[1]\n                    w = w * self.display_size[0] // self.rgb888p_size[0]\n                    h = h * self.display_size[1] // self.rgb888p_size[1]\n                    pl.osd_img.draw_rectangle(x, y, w, h, color=(255, 255, 0, 255), thickness=2)  # \u7ed8\u5236\u77e9\u5f62\u6846\n            else:\n                pl.osd_img.clear()\n\n    # \u83b7\u53d6padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]  # \u6a21\u578b\u8f93\u5165\u5bbd\u5ea6\n        dst_h = self.model_input_size[1]  # \u6a21\u578b\u8f93\u5165\u9ad8\u5ea6\n        ratio_w = dst_w / self.rgb888p_size[0]  # \u5bbd\u5ea6\u7f29\u653e\u6bd4\u4f8b\n        ratio_h = dst_h / self.rgb888p_size[1]  # \u9ad8\u5ea6\u7f29\u653e\u6bd4\u4f8b\n        ratio = min(ratio_w, ratio_h)  # \u53d6\u8f83\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\n        new_w = int(ratio * self.rgb888p_size[0])  # \u65b0\u5bbd\u5ea6\n        new_h = int(ratio * self.rgb888p_size[1])  # \u65b0\u9ad8\u5ea6\n        dw = (dst_w - new_w) / 2  # \u5bbd\u5ea6\u5dee\n        dh = (dst_h - new_h) / 2  # \u9ad8\u5ea6\u5dee\n        top = int(round(0))\n        bottom = int(round(dh * 2 + 0.1))\n        left = int(round(0))\n        right = int(round(dw * 2 - 0.1))\n        return top, bottom, left, right\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[640,360]\n    rgb888p_size = [1920, 1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    kmodel_path = "/sdcard/examples/kmodel/face_detection_320.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    confidence_threshold = 0.5\n    nms_threshold = 0.2\n    anchor_len = 4200\n    det_dim = 4\n    anchors_path = "/sdcard/examples/utils/prior_data_320.bin"\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len, det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n    face_det = FaceDetectionApp(kmodel_path, model_input_size=[320, 320], anchors=anchors, confidence_threshold=confidence_threshold, nms_threshold=nms_threshold, rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    face_det.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = face_det.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                face_det.draw_result(pl, res)   # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        face_det.deinit()                       # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n\n'})}),"\n",(0,s.jsx)(n.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"face_detect",src:i(44658).Z+"",width:"1024",height:"624"})}),"\n",(0,s.jsx)(n.p,{children:"\u200b\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u4e0a\u770b\u5230\u4eba\u8138\u68c0\u6d4b\u7684\u7ed3\u679c\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image-20250423171607912",src:i(62857).Z+"",width:"1374",height:"827"})})]})}function c(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},44658:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/face_detect-1059b55658a4909ba013196f3222a36c.jpg"},62857:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/image-20250423171607912-363485aee6dc5b431176b465a30b510c.png"},11151:(e,n,i)=>{i.d(n,{Z:()=>l,a:()=>r});var s=i(67294);const t={},d=s.createContext(t);function r(e){const n=s.useContext(d);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(d.Provider,{value:n},e.children)}}}]);