"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8791],{10200:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>_});var i=n(85893),t=n(11151);const a={sidebar_position:8},l="\u81ea\u5206\u7c7b\u5b66\u4e60",r={id:"CanaanK230/part14/selfLearning",title:"\u81ea\u5206\u7c7b\u5b66\u4e60",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/03-selfLearning.md",sourceDirName:"CanaanK230/part14",slug:"/CanaanK230/part14/selfLearning",permalink:"/en/CanaanK230/part14/selfLearning",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/03-selfLearning.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_position:8},sidebar:"canaanK230Sidebar",previous:{title:"\u5b9e\u65f6\u8ddf\u8e2a",permalink:"/en/CanaanK230/part14/Nanotracker"},next:{title:"\u591a\u5a92\u4f53\u5e94\u7528",permalink:"/en/category/\u591a\u5a92\u4f53\u5e94\u7528"}},o={},_=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u793a\u4f8b\u4ee3\u7801",id:"2\u793a\u4f8b\u4ee3\u7801",level:2},{value:"3.\u5b9e\u9a8c\u7ed3\u679c",id:"3\u5b9e\u9a8c\u7ed3\u679c",level:2}];function p(e){const s={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h1,{id:"\u81ea\u5206\u7c7b\u5b66\u4e60",children:"\u81ea\u5206\u7c7b\u5b66\u4e60"}),"\n",(0,i.jsx)(s.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,i.jsx)(s.p,{children:"\u5b66\u4e60\u6444\u50cf\u5934\u8fdb\u884c\u81ea\u5b66\u4e60\u8bc6\u522b\u3002"}),"\n",(0,i.jsx)(s.h2,{id:"2\u793a\u4f8b\u4ee3\u7801",children:"2.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube\n\n# \u81ea\u5b9a\u4e49\u81ea\u5b66\u4e60\u7c7b\nclass SelfLearningApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,labels,top_k,threshold,database_path,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        self.labels=labels\n        self.database_path=database_path\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # \u8bc6\u522b\u9608\u503c\n        self.threshold = threshold\n        # \u9009\u62e9top_k\u4e2a\u76f8\u4f3c\u5ea6\u5927\u4e8e\u9608\u503c\u7684\u7ed3\u679c\u7c7b\u522b\n        self.top_k = top_k\n        #\u5bf9\u5e94\u7c7b\u522b\u6ce8\u518c\u7279\u5f81\u6570\u91cf\n        self.features=[2,2]\n        #\u6ce8\u518c\u5355\u4e2a\u7279\u5f81\u4e2d\u9014\u95f4\u9694\u5e27\u6570\n        self.time_one=60\n        self.time_all = 0\n        self.time_now = 0\n        # \u7c7b\u522b\u7d22\u5f15\n        self.category_index = 0\n        # \u7279\u5f81\u5316\u90e8\u5206\u526a\u5207\u5bbd\u9ad8\n        self.crop_w = 400\n        self.crop_h = 400\n        # crop\u7684\u4f4d\u7f6e\n        self.crop_x = self.rgb888p_size[0] / 2.0 - self.crop_w / 2.0\n        self.crop_y = self.rgb888p_size[1] / 2.0 - self.crop_h / 2.0\n        self.crop_x_osd=0\n        self.crop_y_osd=0\n        self.crop_w_osd=0\n        self.crop_h_osd=0\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.data_init()\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.crop(int(self.crop_x),int(self.crop_y),int(self.crop_w),int(self.crop_h))\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0][0]\n\n    # \u7ed8\u5236\u7ed3\u679c\uff0c\u7ed8\u5236\u7279\u5f81\u91c7\u96c6\u6846\u548c\u7279\u5f81\u5206\u7c7b\u6846\n    def draw_result(self,pl,feature):\n        pl.osd_img.clear()\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            pl.osd_img.draw_rectangle(self.crop_x_osd,self.crop_y_osd, self.crop_w_osd, self.crop_h_osd, color=(255, 255, 0, 255), thickness = 4)\n            if (self.category_index < len(self.labels)):\n                self.time_now += 1\n                pl.osd_img.draw_string_advanced(50, self.crop_y_osd-50, 30,"\u8bf7\u5c06\u5f85\u6dfb\u52a0\u7c7b\u522b\u653e\u5165\u6846\u5185\u8fdb\u884c\u7279\u5f81\u91c7\u96c6\uff1a"+self.labels[self.category_index] + "_" + str(int(self.time_now-1) // self.time_one) + ".bin", color=(255,255,0,0))\n                with open(self.database_path + self.labels[self.category_index] + "_" + str(int(self.time_now-1) // self.time_one) + ".bin", \'wb\') as f:\n                    f.write(feature.tobytes())\n                if (self.time_now // self.time_one == self.features[self.category_index]):\n                    self.category_index += 1\n                    self.time_all -= self.time_now\n                    self.time_now = 0\n            else:\n                results_learn = []\n                list_features = os.listdir(self.database_path)\n                for feature_name in list_features:\n                    with open(self.database_path + feature_name, \'rb\') as f:\n                        data = f.read()\n                    save_vec = np.frombuffer(data, dtype=np.float)\n                    score = self.getSimilarity(feature, save_vec)\n                    if (score > self.threshold):\n                        res = feature_name.split("_")\n                        is_same = False\n                        for r in results_learn:\n                            if (r["category"] ==  res[0]):\n                                if (r["score"] < score):\n                                    r["bin_file"] = feature_name\n                                    r["score"] = score\n                                is_same = True\n                        if (not is_same):\n                            if(len(results_learn) < self.top_k):\n                                evec = {}\n                                evec["category"] = res[0]\n                                evec["score"] = score\n                                evec["bin_file"] = feature_name\n                                results_learn.append( evec )\n                                results_learn = sorted(results_learn, key=lambda x: -x["score"])\n                            else:\n                                if( score <= results_learn[self.top_k-1]["score"] ):\n                                    continue\n                                else:\n                                    evec = {}\n                                    evec["category"] = res[0]\n                                    evec["score"] = score\n                                    evec["bin_file"] = feature_name\n                                    results_learn.append( evec )\n                                    results_learn = sorted(results_learn, key=lambda x: -x["score"])\n                                    results_learn.pop()\n                draw_y = 200\n                for r in results_learn:\n                    pl.osd_img.draw_string_advanced( 50 , draw_y,50,r["category"] + " : " + str(r["score"]), color=(255,255,0,0))\n                    draw_y += 50\n\n    #\u6570\u636e\u521d\u59cb\u5316\n    def data_init(self):\n        os.mkdir(self.database_path)\n        self.crop_x_osd = int(self.crop_x / self.rgb888p_size[0] * self.display_size[0])\n        self.crop_y_osd = int(self.crop_y / self.rgb888p_size[1] * self.display_size[1])\n        self.crop_w_osd = int(self.crop_w / self.rgb888p_size[0] * self.display_size[0])\n        self.crop_h_osd = int(self.crop_h / self.rgb888p_size[1] * self.display_size[1])\n        for i in range(len(self.labels)):\n            for j in range(self.features[i]):\n                self.time_all += self.time_one\n\n    # \u83b7\u53d6\u4e24\u4e2a\u7279\u5f81\u5411\u91cf\u7684\u76f8\u4f3c\u5ea6\n    def getSimilarity(self,output_vec,save_vec):\n        tmp = sum(output_vec * save_vec)\n        mold_out = np.sqrt(sum(output_vec * output_vec))\n        mold_save = np.sqrt(sum(save_vec * save_vec))\n        return tmp / (mold_out * mold_save)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[1280,720]\n    rgb888p_size=[1920,1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/examples/kmodel/recognition.kmodel"\n    database_path="/sdcard/examples/utils/features/"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    model_input_size=[224,224]\n    labels=["\u82f9\u679c","\u9999\u8549"]\n    top_k=3\n    threshold=0.5\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b66\u4e60\u5b9e\u4f8b\n    sl=SelfLearningApp(kmodel_path,model_input_size=model_input_size,labels=labels,top_k=top_k,threshold=threshold,database_path=database_path,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    sl.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=sl.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                sl.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        # \u5220\u9664features\u6587\u4ef6\u5939\n        stat_info = os.stat(database_path)\n        if (stat_info[0] & 0x4000):\n            list_files = os.listdir(database_path)\n            for l in list_files:\n                os.remove(database_path + l)\n        os.rmdir(database_path)\n        sl.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(s.h2,{id:"3\u5b9e\u9a8c\u7ed3\u679c",children:"3.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image-20250423183824019",src:n(33341).Z+"",width:"616",height:"611"})}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsxs)(s.p,{children:["\u6ce8\u610f\uff1a\u6bcf\u6b21\u8fd0\u884c\u4ee3\u7801\u540e\u4f1a\u5728\u4e0b\u9762\u8def\u5f84\u6587\u4ef6\u5939\u751f\u6210\u7279\u5f81\u6570\u636e**/sdcard/examples/utils/features/**\uff0c\u5982\u9047\u5230\u610f\u5916\u62a5\u9519\u60c5\u51b5\u53ef\u4ee5\u5220\u9664\u6574\u4e2a",(0,i.jsx)(s.code,{children:"features"}),"\u6587\u4ef6\u5939\u91cd\u65b0\u8fd0\u884c\u4ee3\u7801\u5373\u53ef\u3002"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image-20250423190544804",src:n(90830).Z+"",width:"1464",height:"803"})}),"\n",(0,i.jsx)(s.p,{children:"\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u6839\u636e\u5c4f\u5e55\u63d0\u793a\u82f9\u679c\u548c\u9999\u8549\u7684\u4fe1\u606f\uff1a"}),"\n",(0,i.jsx)(s.p,{children:"\u91c7\u96c6\u82f9\u679c\u7279\u5f81\uff1a"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image-20250423191039023",src:n(62022).Z+"",width:"1403",height:"931"})}),"\n",(0,i.jsx)(s.p,{children:"\u91c7\u96c6\u9999\u8549\u7279\u5f81\uff1a"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image-20250423191102425",src:n(41157).Z+"",width:"1293",height:"1019"})}),"\n",(0,i.jsx)(s.p,{children:"\u8bc6\u522b\u82f9\u679c\uff1a"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image-20250423191048333",src:n(75186).Z+"",width:"1564",height:"1068"})}),"\n",(0,i.jsx)(s.p,{children:"\u8bc6\u522b\u9999\u8549\uff1a"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"image-20250423191055505",src:n(64840).Z+"",width:"1238",height:"878"})})]})}function d(e={}){const{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},33341:(e,s,n)=>{n.d(s,{Z:()=>i});const i=n.p+"assets/images/image-20250423183824019-2115130d3593ca190494d11f54a9ca3e.png"},90830:(e,s,n)=>{n.d(s,{Z:()=>i});const i=n.p+"assets/images/image-20250423190544804-9890bfffcc30875bbeaac6eabb73c0ba.png"},62022:(e,s,n)=>{n.d(s,{Z:()=>i});const i=n.p+"assets/images/image-20250423191039023-88fa537ae4ac16b1bd6899331b924c56.png"},75186:(e,s,n)=>{n.d(s,{Z:()=>i});const i=n.p+"assets/images/image-20250423191048333-319ee10b30389049bc2253a4f441db7a.png"},64840:(e,s,n)=>{n.d(s,{Z:()=>i});const i=n.p+"assets/images/image-20250423191055505-2c71e538ce3d9ae39b17ac60419d19e4.png"},41157:(e,s,n)=>{n.d(s,{Z:()=>i});const i=n.p+"assets/images/image-20250423191102425-5f5ef305572e5279afd99c85d2d99044.png"},11151:(e,s,n)=>{n.d(s,{Z:()=>r,a:()=>l});var i=n(67294);const t={},a=i.createContext(t);function l(e){const s=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(a.Provider,{value:s},e.children)}}}]);