"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2635],{94415:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>t,frontMatter:()=>_,metadata:()=>l,toc:()=>p});var i=s(85893),d=s(11151);const _={sidebar_position:3},r="3D\u4eba\u8138\u7f51\u683c",l={id:"CanaanK230/part14/part2/faceMesh",title:"3D\u4eba\u8138\u7f51\u683c",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part2/03-faceMesh.md",sourceDirName:"CanaanK230/part14/part2",slug:"/CanaanK230/part14/part2/faceMesh",permalink:"/en/CanaanK230/part14/part2/faceMesh",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part2/03-faceMesh.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"canaanK230Sidebar",previous:{title:"\u4eba\u8138\u5173\u952e\u90e8\u4f4d",permalink:"/en/CanaanK230/part14/part2/faceLandmark"},next:{title:"\u4eba\u8138\u6ce8\u89c6\u65b9\u5411\u68c0\u6d4b",permalink:"/en/CanaanK230/part14/part2/eyeGaze"}},a={},p=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u6838\u5fc3\u4ee3\u7801",id:"2\u6838\u5fc3\u4ee3\u7801",level:2},{value:"\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b",id:"\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b",level:3},{value:"\u521d\u59cb\u5316\u6210\u5458\u53d8\u91cf",id:"\u521d\u59cb\u5316\u6210\u5458\u53d8\u91cf",level:3},{value:"\u9884\u5904\u7406",id:"\u9884\u5904\u7406",level:3},{value:"\u540e\u5904\u7406",id:"\u540e\u5904\u7406",level:3},{value:"\u4eba\u8138\u7f51\u683c\u68c0\u6d4b\u4efb\u52a1\u7c7b",id:"\u4eba\u8138\u7f51\u683c\u68c0\u6d4b\u4efb\u52a1\u7c7b",level:3},{value:"\u9884\u5904\u7406",id:"\u9884\u5904\u7406-1",level:3},{value:"ROI\u8ba1\u7b97",id:"roi\u8ba1\u7b97",level:3},{value:"\u540e\u5904\u7406",id:"\u540e\u5904\u7406-1",level:3},{value:"\u5b8c\u6574\u6d41\u7a0b",id:"\u5b8c\u6574\u6d41\u7a0b",level:3},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function o(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,d.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"3d\u4eba\u8138\u7f51\u683c",children:"3D\u4eba\u8138\u7f51\u683c"}),"\n",(0,i.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,i.jsx)(n.p,{children:"\u8fdb\u884c3D\u4eba\u8138\u7f51\u683c\u7684\u4f30\u8ba1\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"2\u6838\u5fc3\u4ee3\u7801",children:"2.\u6838\u5fc3\u4ee3\u7801"}),"\n",(0,i.jsx)(n.h3,{id:"\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b",children:"\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b"}),"\n",(0,i.jsxs)(n.p,{children:["\u7c7b\u521d\u59cb\u5316 ",(0,i.jsx)(n.code,{children:"__init__"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"def __init__(self, kmodel_path, model_input_size, anchors, confidence_threshold=0.25, nms_threshold=0.3, rgb888p_size=[1280,720], display_size=[1920,1080], debug_mode=0):\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u4e3b\u8981\u53c2\u6570\u89e3\u91ca\uff1a"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"kmodel_path"}),"\uff1a\u6a21\u578b\u8def\u5f84"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_input_size"}),"\uff1a\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\uff08\u5982 ",(0,i.jsx)(n.code,{children:"[320, 240]"}),"\uff09"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"anchors"}),"\uff1aYOLO \u7c7b\u578b\u6a21\u578b\u4f7f\u7528\u7684\u951a\u6846"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"confidence_threshold"})," \u548c ",(0,i.jsx)(n.code,{children:"nms_threshold"}),"\uff1a\u7f6e\u4fe1\u5ea6\u548c\u975e\u6781\u5927\u503c\u6291\u5236\u9608\u503c"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"rgb888p_size"}),"\uff1aSensor \u91c7\u96c6\u56fe\u50cf\u7684\u5206\u8fa8\u7387"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"display_size"}),"\uff1a\u6700\u7ec8\u663e\u793a\u5206\u8fa8\u7387"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"debug_mode"}),"\uff1a\u662f\u5426\u5f00\u542f\u8c03\u8bd5"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u521d\u59cb\u5316\u6210\u5458\u53d8\u91cf",children:"\u521d\u59cb\u5316\u6210\u5458\u53d8\u91cf"}),"\n",(0,i.jsxs)(n.p,{children:["\u5305\u62ec\u5bf9 ",(0,i.jsx)(n.code,{children:"Ai2d"})," \u5b9e\u4f8b\u5316\u548c\u8bbe\u7f6e\u683c\u5f0f\u7c7b\u578b\uff1a"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"self.ai2d = Ai2d(debug_mode)\nself.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u8868\u793a\u56fe\u50cf\u9884\u5904\u7406\u8f93\u5165\u8f93\u51fa\u4e3a ",(0,i.jsx)(n.code,{children:"NCHW"})," \u683c\u5f0f\uff0c\u6570\u636e\u7c7b\u578b\u4e3a ",(0,i.jsx)(n.code,{children:"uint8"}),"\u3002"]}),"\n",(0,i.jsx)(n.h3,{id:"\u9884\u5904\u7406",children:"\u9884\u5904\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\nself.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\nself.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"pad()"})," \u589e\u52a0\u8fb9\u7f18\u7a7a\u767d\u4ee5\u4fdd\u6301\u56fe\u50cf\u5bbd\u9ad8\u6bd4\uff0cpad \u503c\u7531 ",(0,i.jsx)(n.code,{children:"get_pad_param()"})," \u81ea\u52a8\u8ba1\u7b97"]}),"\n",(0,i.jsxs)(n.li,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"resize()"})," \u5c06\u56fe\u50cf\u7f29\u653e\u5230\u6a21\u578b\u8f93\u5165\u5927\u5c0f"]}),"\n",(0,i.jsxs)(n.li,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"build()"})," \u6784\u5efa\u6574\u4e2a\u9884\u5904\u7406\u6d41\u7a0b\u56fe"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Padding\u53c2\u6570\u8ba1\u7b97 ",(0,i.jsx)(n.code,{children:"get_pad_param()"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ratio = min(dst_w / src_w, dst_h / src_h)\nnew_w = ratio * src_w\nnew_h = ratio * src_h\ndw = dst_w - new_w\ndh = dst_h - new_h\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u6b64\u51fd\u6570\u6309\u6bd4\u4f8b\u7f29\u653e\u56fe\u50cf\uff0c\u518d\u5bf9\u8fb9\u7f18\u52a0 padding \u4fdd\u6301\u5c45\u4e2d\uff0c\u907f\u514d\u56fe\u50cf\u53d8\u5f62\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u540e\u5904\u7406",children:"\u540e\u5904\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"aidemo.face_det_post_process()"})," \u662f CanMV \u63d0\u4f9b\u7684 C \u5e93\u51fd\u6570\uff0c\u5904\u7406 YOLO \u68c0\u6d4b\u6846\u8f93\u51fa\uff0c\u5305\u62ec\u89e3\u7801\u3001\u8fc7\u6ee4\u4f4e\u5206\u6570\u6846\u3001NMS \u7b49\u3002"]}),"\n",(0,i.jsxs)(n.li,{children:["\u8f93\u51fa\u7ed3\u679c\u662f\u591a\u4e2a\u68c0\u6d4b\u6846\uff0c\u82e5\u6709\u591a\u4e2a\u7ed3\u679c\u8fd4\u56de ",(0,i.jsx)(n.code,{children:"res[0]"})," \u4f5c\u4e3a\u4e3b\u7ed3\u679c\u3002"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u4eba\u8138\u7f51\u683c\u68c0\u6d4b\u4efb\u52a1\u7c7b",children:"\u4eba\u8138\u7f51\u683c\u68c0\u6d4b\u4efb\u52a1\u7c7b"}),"\n",(0,i.jsx)(n.p,{children:"\u8fd9\u4e2a\u7c7b\u7684\u4efb\u52a1\u662f\u5728\u4eba\u8138\u6846\u4e2d\u63d0\u53d6\u5173\u952e\u70b9\u53c2\u6570\uff08\u4e00\u822c\u662f 3D \u59ff\u6001\u6216 3D \u7f51\u683c\uff09\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u521d\u59cb\u5316\u65f6\u52a0\u8f7d\u5747\u503c/\u65b9\u5dee\u53c2\u6570"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"self.param_mean = np.array([...])\nself.param_std = np.array([...])\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u8fd9\u4e9b\u662f\u7528\u4e8e\u6a21\u578b\u8f93\u51fa\u53cd\u5f52\u4e00\u5316\u7684\u53c2\u6570\uff0cYOLOv8-face-mesh \u6216\u7c7b\u4f3c\u6a21\u578b\u7684\u8f93\u51fa\u4e00\u822c\u662f\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u4eba\u8138\u53c2\u6570\u5411\u91cf\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u9884\u5904\u7406-1",children:"\u9884\u5904\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"roi = self.parse_roi_box_from_bbox(det)\nself.ai2d.crop(int(roi[0]),int(roi[1]),int(roi[2]),int(roi[3]))\nself.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\nself.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\u6839\u636e\u4eba\u8138\u6846 ",(0,i.jsx)(n.code,{children:"det"})," \u63d0\u53d6 ",(0,i.jsx)(n.code,{children:"roi"})," \u533a\u57df"]}),"\n",(0,i.jsxs)(n.li,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"crop()"})," \u622a\u53d6\u8be5\u533a\u57df\u5e76\u505a ",(0,i.jsx)(n.code,{children:"resize"}),"\uff0c\u6784\u5efa\u6a21\u578b\u8f93\u5165\u6570\u636e\u6d41"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"roi\u8ba1\u7b97",children:"ROI\u8ba1\u7b97"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"old_size = (w + h) / 2\ncenter_x = x + w/2\ncenter_y = y + h/2 + old_size * 0.14\nsize = old_size * 1.58\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u8be5\u65b9\u6cd5\u662f FaceMesh \u8bba\u6587\u4e2d\u6807\u51c6\u7684\u4eba\u8138\u88c1\u526a\u65b9\u5f0f\uff0c\u4e2d\u5fc3\u7a0d\u5fae\u504f\u4e0b\uff0c\u533a\u57df\u7a0d\u5fae\u653e\u5927\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u540e\u5904\u7406-1",children:"\u540e\u5904\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"param = results[0] * self.param_std + self.param_mean\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u6a21\u578b\u8f93\u51fa\u662f\u4e00\u4e2a\u5f52\u4e00\u5316\u5411\u91cf\uff0c\u5bf9\u5b83\u8fdb\u884c\u53cd\u5f52\u4e00\u5316\u4ee5\u83b7\u5f97\u771f\u5b9e\u7684\u4eba\u8138\u59ff\u6001\u53c2\u6570\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u5b8c\u6574\u6d41\u7a0b",children:"\u5b8c\u6574\u6d41\u7a0b"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"FaceDetApp"})," \u5904\u7406\u56fe\u50cf\u5f97\u5230\u4eba\u8138\u6846\uff1a"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"face_bbox = face_det_app.postprocess(results_from_model)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"FaceMeshApp"})," \u8bfb\u53d6\u8be5\u4eba\u8138\u6846\uff0c\u9884\u5904\u7406\u5e76\u8fdb\u884c\u63a8\u7406\uff1a"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"roi = face_mesh_app.config_preprocess(face_bbox)\nface_param = face_mesh_app.postprocess(results_from_mesh_model)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u68c0\u6d4b\u4efb\u52a1\u951a\u6846\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u63a8\u7406\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # padding\u53c2\u6570\u8ba1\u7b97\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u7f51\u683c\u4efb\u52a1\u7c7b\nclass FaceMeshApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u7f51\u683c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u4eba\u8138mesh\u53c2\u6570\u5747\u503c\n        self.param_mean = np.array([0.0003492636315058917,2.52790130161884e-07,-6.875197868794203e-07,60.1679573059082,-6.295513230725192e-07,0.0005757200415246189,-5.085391239845194e-05,74.2781982421875,5.400917189035681e-07,6.574138387804851e-05,0.0003442012530285865,-66.67157745361328,-346603.6875,-67468.234375,46822.265625,-15262.046875,4350.5888671875,-54261.453125,-18328.033203125,-1584.328857421875,-84566.34375,3835.960693359375,-20811.361328125,38094.9296875,-19967.85546875,-9241.3701171875,-19600.71484375,13168.08984375,-5259.14404296875,1848.6478271484375,-13030.662109375,-2435.55615234375,-2254.20654296875,-14396.5615234375,-6176.3291015625,-25621.919921875,226.39447021484375,-6326.12353515625,-10867.2509765625,868.465087890625,-5831.14794921875,2705.123779296875,-3629.417724609375,2043.9901123046875,-2446.6162109375,3658.697021484375,-7645.98974609375,-6674.45263671875,116.38838958740234,7185.59716796875,-1429.48681640625,2617.366455078125,-1.2070955038070679,0.6690792441368103,-0.17760828137397766,0.056725528091192245,0.03967815637588501,-0.13586315512657166,-0.09223993122577667,-0.1726071834564209,-0.015804484486579895,-0.1416848599910736],dtype=np.float)\n        # \u4eba\u8138mesh\u53c2\u6570\u65b9\u5dee\n        self.param_std = np.array([0.00017632152594160289,6.737943476764485e-05,0.00044708489440381527,26.55023193359375,0.0001231376954820007,4.493021697271615e-05,7.923670636955649e-05,6.982563018798828,0.0004350444069132209,0.00012314890045672655,0.00017400001524947584,20.80303955078125,575421.125,277649.0625,258336.84375,255163.125,150994.375,160086.109375,111277.3046875,97311.78125,117198.453125,89317.3671875,88493.5546875,72229.9296875,71080.2109375,50013.953125,55968.58203125,47525.50390625,49515.06640625,38161.48046875,44872.05859375,46273.23828125,38116.76953125,28191.162109375,32191.4375,36006.171875,32559.892578125,25551.1171875,24267.509765625,27521.3984375,23166.53125,21101.576171875,19412.32421875,19452.203125,17454.984375,22537.623046875,16174.28125,14671.640625,15115.6884765625,13870.0732421875,13746.3125,12663.1337890625,1.5870834589004517,1.5077009201049805,0.5881357789039612,0.5889744758605957,0.21327851712703705,0.2630201280117035,0.2796429395675659,0.38030216097831726,0.16162841022014618,0.2559692859649658],dtype=np.float)\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97crop\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6ecrop\u9884\u5904\u7406\n            roi = self.parse_roi_box_from_bbox(det)\n            self.ai2d.crop(int(roi[0]),int(roi[1]),int(roi[2]),int(roi[3]))\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            return roi\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            param = results[0] * self.param_std + self.param_mean\n            return param\n\n    def parse_roi_box_from_bbox(self,bbox):\n        # \u83b7\u53d6\u4eba\u8138roi\n        x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n        old_size = (w + h) / 2\n        center_x = x1 + w / 2\n        center_y = y1 + h / 2 + old_size * 0.14\n        size = int(old_size * 1.58)\n        x0 = center_x - float(size) / 2\n        y0 = center_y - float(size) / 2\n        x1 = x0 + size\n        y1 = y0 + size\n        x0 = max(0, min(x0, self.rgb888p_size[0]))\n        y0 = max(0, min(y0, self.rgb888p_size[1]))\n        x1 = max(0, min(x1, self.rgb888p_size[0]))\n        y1 = max(0, min(y1, self.rgb888p_size[1]))\n        roi = (x0, y0, x1 - x0, y1 - y0)\n        return roi\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\u4efb\u52a1\u7c7b\nclass FaceMeshPostApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u7f51\u683c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u91cd\u5199\u9884\u5904\u7406\u51fd\u6570preprocess\uff0c\u56e0\u4e3a\u8be5\u6a21\u578b\u7684\u9884\u5904\u7406\u4e0d\u662f\u5355\u7eaf\u8c03\u7528\u4e00\u4e2aai2d\u80fd\u5b9e\u73b0\u7684\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165\u7684tensor\u5217\u8868\n    def preprocess(self,param):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # face mesh post\u6a21\u578b\u9884\u5904\u7406\uff0cparam\u89e3\u6790\n            param = param[0]\n            trans_dim, shape_dim, exp_dim = 12, 40, 10\n            R_ = param[:trans_dim].copy().reshape((3, -1))\n            R = R_[:, :3].copy()\n            offset = R_[:, 3].copy()\n            offset = offset.reshape((3, 1))\n            alpha_shp = param[trans_dim:trans_dim + shape_dim].copy().reshape((-1, 1))\n            alpha_exp = param[trans_dim + shape_dim:].copy().reshape((-1, 1))\n            R_tensor = nn.from_numpy(R)\n            offset_tensor = nn.from_numpy(offset)\n            alpha_shp_tensor = nn.from_numpy(alpha_shp)\n            alpha_exp_tensor = nn.from_numpy(alpha_exp)\n            return [R_tensor,offset_tensor,alpha_shp_tensor,alpha_exp_tensor]\n\n    # \u81ea\u5b9a\u4e49\u6a21\u578b\u540e\u5904\u7406\uff0c\u8fd9\u91cc\u8c03\u7528\u4e86aidemo\u7684face_mesh_post_process\u63a5\u53e3\n    def postprocess(self,results,roi):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            x, y, w, h = map(lambda x: int(round(x, 0)), roi[:4])\n            x = x * self.display_size[0] // self.rgb888p_size[0]\n            y = y * self.display_size[1] // self.rgb888p_size[1]\n            w = w * self.display_size[0] // self.rgb888p_size[0]\n            h = h * self.display_size[1] // self.rgb888p_size[1]\n            roi_array = np.array([x,y,w,h],dtype=np.float)\n            aidemo.face_mesh_post_process(roi_array,results[0])\n            return results[0]\n\n# 3D\u4eba\u8138\u7f51\u683c\nclass FaceMesh:\n    def __init__(self,face_det_kmodel,face_mesh_kmodel,mesh_post_kmodel,det_input_size,mesh_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u81383D\u7f51\u683c\u6a21\u578b\u8def\u5f84\n        self.face_mesh_kmodel=face_mesh_kmodel\n        # \u4eba\u81383D\u7f51\u683c\u540e\u5904\u7406\u6a21\u578b\u8def\u5f84\n        self.mesh_post_kmodel=mesh_post_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u81383D\u7f51\u683c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.mesh_input_size=mesh_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        # \u4eba\u8138\u7f51\u683c\u5b9e\u4f8b\n        self.face_mesh=FaceMeshApp(self.face_mesh_kmodel,model_input_size=self.mesh_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\u5b9e\u4f8b\n        self.face_mesh_post=FaceMeshPostApp(self.mesh_post_kmodel,model_input_size=self.mesh_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u4eba\u8138\u68c0\u6d4b\u9884\u5904\u7406\u914d\u7f6e\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        mesh_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u4eba\u8138\u914d\u7f6e\u9884\u5904\u7406\uff0c\u6267\u884c\u4eba\u8138\u7f51\u683c\u548c\u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\n            roi=self.face_mesh.config_preprocess(det_box)\n            param=self.face_mesh.run(input_np)\n            tensors=self.face_mesh_post.preprocess(param)\n            results=self.face_mesh_post.inference(tensors)\n            res=self.face_mesh_post.postprocess(results,roi)\n            mesh_res.append(res)\n        return det_boxes,mesh_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u89e3\u6790\u6548\u679c\n    def draw_result(self,pl,dets,mesh_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            for vertices in mesh_res:\n                aidemo.face_draw_mesh(draw_img_np, vertices)\n            pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\uff0ck230d\u53d7\u9650\u4e8e\u5185\u5b58\u4e0d\u652f\u6301\n    display_mode="lcd"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/examples/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u7f51\u683c\u6a21\u578b\u8def\u5f84\n    face_mesh_kmodel_path="/sdcard/examples/kmodel/face_alignment.kmodel"\n    # \u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\u6a21\u578b\u8def\u5f84\n    face_mesh_post_kmodel_path="/sdcard/examples/kmodel/face_alignment_post.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    face_mesh_input_size=[120,120]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    fm=FaceMesh(face_det_kmodel_path,face_mesh_kmodel_path,face_mesh_post_kmodel_path,det_input_size=face_det_input_size,mesh_input_size=face_mesh_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,mesh_res=fm.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                fm.draw_result(pl,det_boxes,mesh_res)   # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u663e\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fm.face_det.deinit()\n        fm.face_mesh.deinit()\n        fm.face_mesh_post.deinit()\n        pl.destroy()\n\n\n'})}),"\n",(0,i.jsx)(n.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250423171814319",src:s(69670).Z+"",width:"416",height:"239"})}),"\n",(0,i.jsx)(n.p,{children:"\u200b\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u4e0a\u770b\u52303D\u4eba\u8138\u7f51\u683c\u7684\u7ed3\u679c\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250423172233191",src:s(90672).Z+"",width:"1401",height:"966"})})]})}function t(e={}){const{wrapper:n}={...(0,d.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},69670:(e,n,s)=>{s.d(n,{Z:()=>i});const i=s.p+"assets/images/image-20250423171814319-411480e15f281f7ce43edb04d8412743.png"},90672:(e,n,s)=>{s.d(n,{Z:()=>i});const i=s.p+"assets/images/image-20250423172233191-3bad7bf100f7d9e57a9d1bc05ccabc3f.png"},11151:(e,n,s)=>{s.d(n,{Z:()=>l,a:()=>r});var i=s(67294);const d={},_=i.createContext(d);function r(e){const n=i.useContext(_);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:r(e.components),i.createElement(_.Provider,{value:n},e.children)}}}]);