"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3845],{8323:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>_,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>t});var i=s(85893),d=s(11151);const r={sidebar_position:2},_="\u4eba\u8138\u5173\u952e\u90e8\u4f4d",l={id:"CanaanK230/part14/part2/faceLandmark",title:"\u4eba\u8138\u5173\u952e\u90e8\u4f4d",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part2/02-faceLandmark.md",sourceDirName:"CanaanK230/part14/part2",slug:"/CanaanK230/part14/part2/faceLandmark",permalink:"/en/CanaanK230/part14/part2/faceLandmark",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part2/02-faceLandmark.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"canaanK230Sidebar",previous:{title:"\u4eba\u8138\u68c0\u6d4b",permalink:"/en/CanaanK230/part14/part2/faceDetection"},next:{title:"3D\u4eba\u8138\u7f51\u683c",permalink:"/en/CanaanK230/part14/part2/faceMesh"}},a={},t=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u6838\u5fc3\u4ee3\u7801",id:"2\u6838\u5fc3\u4ee3\u7801",level:2},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function o(n){const e={code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.a)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"\u4eba\u8138\u5173\u952e\u90e8\u4f4d",children:"\u4eba\u8138\u5173\u952e\u90e8\u4f4d"}),"\n",(0,i.jsx)(e.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,i.jsx)(e.p,{children:"\u5b66\u4e60\u6444\u50cf\u5934\u83b7\u53d6\u7684\u56fe\u50cf\u4e2d\u4eba\u8138\u5173\u952e\u90e8\u4f4d\u7684\u83b7\u53d6\u3002"}),"\n",(0,i.jsx)(e.h2,{id:"2\u6838\u5fc3\u4ee3\u7801",children:"2.\u6838\u5fc3\u4ee3\u7801"}),"\n",(0,i.jsxs)(e.p,{children:["\u672c\u4ee3\u7801\u5b9e\u73b0\u4e86\u4e00\u4e2a",(0,i.jsx)(e.strong,{children:"\u53cc\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u7684\u4eba\u8138\u5173\u952e\u70b9\u68c0\u6d4b\u7cfb\u7edf"}),"\uff0c\u5305\u542b\u4eba\u8138\u68c0\u6d4b\uff08FaceDetApp\uff09\u548c\u5173\u952e\u70b9\u5b9a\u4f4d\uff08FaceLandMarkApp\uff09\u4e24\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7FaceLandMark\u7c7b\u6574\u5408\u6d41\u7a0b\u3002\u7cfb\u7edf\u67b6\u6784\u56fe\u5982\u4e0b\uff1a"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u56fe\u50cf\u8f93\u5165 \u2192 Pipeline\u83b7\u53d6\u5e27 \u2192 \u4eba\u8138\u68c0\u6d4b \u2192 \u5173\u952e\u70b9\u68c0\u6d4b \u2192 \u7ed3\u679c\u7ed8\u5236 \u2192 HDMI/LCD\u663e\u793a\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"1. FaceDetApp\uff08\u4eba\u8138\u68c0\u6d4b\uff09"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u9884\u5904\u7406"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u7b49\u6bd4\u4f8b\u586b\u5145"}),"\uff1a\u901a\u8fc7",(0,i.jsx)(e.code,{children:"get_pad_param()"}),"\u8ba1\u7b97\u586b\u5145\u53c2\u6570\uff0c\u4fdd\u6301\u5bbd\u9ad8\u6bd4\uff08\u907f\u514d\u53d8\u5f62\uff09"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u7f29\u653e\u81f3320x320"}),"\uff1a\u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u7f29\u653e\u81f3\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"# \u8ba1\u7b97\u586b\u5145\u53c2\u6570\u793a\u4f8b\nratio = min(320/\u539f\u56fe\u5bbd, 320/\u539f\u56fe\u9ad8)\nnew_w = \u539f\u56fe\u5bbd*ratio \u2192 \u586b\u5145\u5de6\u53f3\u4f7f\u603b\u5bbd=320\n"})}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u540e\u5904\u7406"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\u8c03\u7528",(0,i.jsx)(e.code,{children:"aidemo.face_det_post_process"}),"\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548cNMS"]}),"\n",(0,i.jsxs)(e.li,{children:["\u8fd4\u56de\u683c\u5f0f\uff1a",(0,i.jsx)(e.code,{children:"[x1,y1,w,h,conf]"}),"\u7684\u68c0\u6d4b\u6846\u5217\u8868"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"2. FaceLandMarkApp\uff08\u5173\u952e\u70b9\u68c0\u6d4b\uff09"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\u6839\u636e\u4eba\u8138\u6846\u8ba1\u7b97",(0,i.jsx)(e.strong,{children:"\u4eff\u5c04\u77e9\u9635"}),"\uff0c\u5c06\u4eba\u8138\u533a\u57df\u5bf9\u9f50\u5230192x192\u8f93\u5165"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"matrix_dst = [\n  [scale, 0, \u5e73\u79fbX],\n  [0, scale, \u5e73\u79fbY]\n]\n"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\u901a\u8fc7",(0,i.jsx)(e.code,{children:"affine()"}),"\u5b9e\u73b0\u65cb\u8f6c+\u7f29\u653e+\u5e73\u79fb\u7684\u590d\u5408\u53d8\u6362"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u5173\u952e\u70b9\u540e\u5904\u7406"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u5750\u6807\u53cd\u53d8\u6362"}),"\uff1a\u4f7f\u7528\u9006\u77e9\u9635\u5c06\u5173\u952e\u70b9\u6620\u5c04\u56de\u539f\u56fe\u5750\u6807"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"new_x = old_x * inv[0] + old_y * inv[1] + inv[2]\nnew_y = old_x * inv[3] + old_y * inv[4] + inv[5]\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"3. FaceLandMark\uff08\u6574\u5408\u7c7b\uff09"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u591a\u90e8\u4f4d\u5173\u952e\u70b9\u7ed8\u5236"}),"\uff1a","\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u5212\u520610\u4e2a\u9762\u90e8\u533a\u57df\uff08\u7709\u6bdb/\u773c\u775b/\u5634\u5507\u7b49\uff09"}),"\n",(0,i.jsxs)(e.li,{children:["\u4f7f\u7528\u4e0d\u540c\u7ed8\u56fe\u65b9\u5f0f\uff1a","\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u8f6e\u5ed3\u7ebf"}),"\uff1a",(0,i.jsx)(e.code,{children:"aidemo.contours()"}),"\u753b\u7709\u6bdb/\u5634\u5507"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u6298\u7ebf"}),"\uff1a",(0,i.jsx)(e.code,{children:"aidemo.polylines()"}),"\u753b\u9f3b\u6881"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u5706\u70b9"}),"\uff1a",(0,i.jsx)(e.code,{children:"draw_circle()"}),"\u753b\u77b3\u5b54"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"\u4eff\u5c04\u77e9\u9635\u8ba1\u7b97"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"def get_affine_matrix(bbox):\n    scale = 192 / (max(w,h)*1.5)  # \u4fdd\u75591.5\u500d\u8fb9\u7f18\u4e0a\u4e0b\u6587\n    cx = (x1 + w/2) * scale       # \u539f\u56fe\u4e2d\u5fc3\u70b9\u6620\u5c04\n    cy = (y1 + h/2) * scale\n    matrix_dst[0,2] = 96 - cx     # \u5e73\u79fb\u5230\u6a21\u578b\u4e2d\u5fc3(96=192/2)\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"\u5173\u952e\u70b9\u9006\u53d8\u6362"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"# \u9006\u53d8\u6362\u6d41\u7a0b\n1. \u5173\u952e\u70b9\u8f93\u51fa\u503c\u57df[-1,1] \u2192 \u8f6c\u6362\u5230[0,192]\n   pred[i] = (pred[i]+1)*96\n2. \u901a\u8fc7\u9006\u77e9\u9635\u6620\u5c04\u56de\u539f\u56fe\u5750\u6807\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"\u591a\u7ebf\u7a0b\u4f18\u5316"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"ScopedTiming"}),"\uff1a\u4ee3\u7801\u4e2d\u591a\u5904\u4f7f\u7528\u6027\u80fd\u8ba1\u65f6\u5668\uff0c\u4fbf\u4e8e\u4f18\u5316\u74f6\u9888"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u5185\u5b58\u7ba1\u7406"}),"\uff1a\u663e\u5f0f\u8c03\u7528",(0,i.jsx)(e.code,{children:"gc.collect()"}),"\u5e94\u5bf9\u5d4c\u5165\u5f0f\u5185\u5b58\u9650\u5236"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["\u5047\u8bbe\u68c0\u6d4b\u5230\u4e00\u4e2a\u4eba\u8138\u6846",(0,i.jsx)(e.code,{children:"(x=100,y=200,w=300,h=300)"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u5173\u952e\u70b9\u9884\u5904\u7406"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"\u8ba1\u7b97scale=192/(300*1.5)=0.426"}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"\u751f\u6210\u4eff\u5c04\u77e9\u9635\uff1a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"[[0.426, 0, 96-127.5],\n [0, 0.426, 96-212.5]] \n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u5173\u952e\u70b9\u8f93\u51fa"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u6a21\u578b\u8f93\u51fa\u5f52\u4e00\u5316\u5750\u6807 \u2192 \u53cd\u5f52\u4e00\u5316\u5230192x192\u7a7a\u95f4"}),"\n",(0,i.jsx)(e.li,{children:"\u901a\u8fc7\u9006\u77e9\u9635\u6620\u5c04\u56de1080P\u539f\u56fe\u5750\u6807"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u68c0\u6d4b\u4efb\u52a1\u951a\u6846\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u7684face_det_post_process\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u5173\u952e\u70b9\u4efb\u52a1\u7c7b\nclass FaceLandMarkApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u76ee\u6807\u77e9\u9635\n        self.matrix_dst=None\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97\u76ee\u6807\u77e9\u9635\uff0c\u5e76\u83b7\u53d6\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n            self.matrix_dst = self.get_affine_matrix(det)\n            affine_matrix = [self.matrix_dst[0][0],self.matrix_dst[0][1],self.matrix_dst[0][2],\n                             self.matrix_dst[1][0],self.matrix_dst[1][1],self.matrix_dst[1][2]]\n            # \u8bbe\u7f6e\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684invert_affine_transform\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            pred=results[0]\n            # \uff081\uff09\u5c06\u4eba\u8138\u5173\u952e\u70b9\u8f93\u51fa\u53d8\u6362\u6a21\u578b\u8f93\u5165\n            half_input_len = self.model_input_size[0] // 2\n            pred = pred.flatten()\n            for i in range(len(pred)):\n                pred[i] += (pred[i] + 1) * half_input_len\n            # \uff082\uff09\u83b7\u53d6\u4eff\u5c04\u77e9\u9635\u7684\u9006\u77e9\u9635\n            matrix_dst_inv = aidemo.invert_affine_transform(self.matrix_dst)\n            matrix_dst_inv = matrix_dst_inv.flatten()\n            # \uff083\uff09\u5bf9\u6bcf\u4e2a\u5173\u952e\u70b9\u8fdb\u884c\u9006\u53d8\u6362\n            half_out_len = len(pred) // 2\n            for kp_id in range(half_out_len):\n                old_x = pred[kp_id * 2]\n                old_y = pred[kp_id * 2 + 1]\n                # \u9006\u53d8\u6362\u516c\u5f0f\n                new_x = old_x * matrix_dst_inv[0] + old_y * matrix_dst_inv[1] + matrix_dst_inv[2]\n                new_y = old_x * matrix_dst_inv[3] + old_y * matrix_dst_inv[4] + matrix_dst_inv[5]\n                pred[kp_id * 2] = new_x\n                pred[kp_id * 2 + 1] = new_y\n            return pred\n\n    def get_affine_matrix(self,bbox):\n        # \u83b7\u53d6\u4eff\u5c04\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u4ece\u8fb9\u754c\u6846\u63d0\u53d6\u5750\u6807\u548c\u5c3a\u5bf8\n            x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n            # \u8ba1\u7b97\u7f29\u653e\u6bd4\u4f8b\uff0c\u4f7f\u5f97\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e00\u90e8\u5206\n            scale_ratio = (self.model_input_size[0]) / (max(w, h) * 1.5)\n            # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u5728\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u5750\u6807\n            cx = (x1 + w / 2) * scale_ratio\n            cy = (y1 + h / 2) * scale_ratio\n            # \u8ba1\u7b97\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e00\u534a\u957f\u5ea6\n            half_input_len = self.model_input_size[0] / 2\n            # \u521b\u5efa\u4eff\u5c04\u77e9\u9635\u5e76\u8fdb\u884c\u8bbe\u7f6e\n            matrix_dst = np.zeros((2, 3), dtype=np.float)\n            matrix_dst[0, 0] = scale_ratio\n            matrix_dst[0, 1] = 0\n            matrix_dst[0, 2] = half_input_len - cx\n            matrix_dst[1, 0] = 0\n            matrix_dst[1, 1] = scale_ratio\n            matrix_dst[1, 2] = half_input_len - cy\n            return matrix_dst\n\n# \u4eba\u8138\u6807\u5fd7\u89e3\u6790\nclass FaceLandMark:\n    def __init__(self,face_det_kmodel,face_landmark_kmodel,det_input_size,landmark_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u6807\u5fd7\u89e3\u6790\u6a21\u578b\u8def\u5f84\n        self.face_landmark_kmodel=face_landmark_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u6807\u5fd7\u89e3\u6790\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.landmark_input_size=landmark_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n\n        # \u4eba\u8138\u5173\u952e\u70b9\u4e0d\u540c\u90e8\u4f4d\u5173\u952e\u70b9\u5217\u8868\n        self.dict_kp_seq = [\n            [43, 44, 45, 47, 46, 50, 51, 49, 48],              # left_eyebrow\n            [97, 98, 99, 100, 101, 105, 104, 103, 102],        # right_eyebrow\n            [35, 36, 33, 37, 39, 42, 40, 41],                  # left_eye\n            [89, 90, 87, 91, 93, 96, 94, 95],                  # right_eye\n            [34, 88],                                          # pupil\n            [72, 73, 74, 86],                                  # bridge_nose\n            [77, 78, 79, 80, 85, 84, 83],                      # wing_nose\n            [52, 55, 56, 53, 59, 58, 61, 68, 67, 71, 63, 64],  # out_lip\n            [65, 54, 60, 57, 69, 70, 62, 66],                  # in_lip\n            [1, 9, 10, 11, 12, 13, 14, 15, 16, 2, 3, 4, 5, 6, 7, 8, 0, 24, 23, 22, 21, 20, 19, 18, 32, 31, 30, 29, 28, 27, 26, 25, 17]  # basin\n        ]\n\n        # \u4eba\u8138\u5173\u952e\u70b9\u4e0d\u540c\u90e8\u4f4d\uff08\u987a\u5e8f\u540cdict_kp_seq\uff09\u989c\u8272\u914d\u7f6e\uff0cargb\n        self.color_list_for_osd_kp = [\n            (255, 0, 255, 0),\n            (255, 0, 255, 0),\n            (255, 255, 0, 255),\n            (255, 255, 0, 255),\n            (255, 255, 0, 0),\n            (255, 255, 170, 0),\n            (255, 255, 255, 0),\n            (255, 0, 255, 255),\n            (255, 255, 220, 50),\n            (255, 30, 30, 255)\n        ]\n        # \u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        # \u4eba\u8138\u6807\u5fd7\u89e3\u6790\u5b9e\u4f8b\n        self.face_landmark=FaceLandMarkApp(self.face_landmark_kmodel,model_input_size=self.landmark_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u914d\u7f6e\u4eba\u8138\u68c0\u6d4b\u7684\u9884\u5904\u7406\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        landmark_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u5230\u7684\u4eba\u8138\u89e3\u6790\u5173\u952e\u90e8\u4f4d\n            self.face_landmark.config_preprocess(det_box)\n            res=self.face_landmark.run(input_np)\n            landmark_res.append(res)\n        return det_boxes,landmark_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u89e3\u6790\u6548\u679c\n    def draw_result(self,pl,dets,landmark_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            for pred in landmark_res:\n                # \uff081\uff09\u83b7\u53d6\u5355\u4e2a\u4eba\u8138\u6846\u5bf9\u5e94\u7684\u4eba\u8138\u5173\u952e\u70b9\n                for sub_part_index in range(len(self.dict_kp_seq)):\n                    # \uff082\uff09\u6784\u5efa\u4eba\u8138\u67d0\u4e2a\u533a\u57df\u5173\u952e\u70b9\u96c6\n                    sub_part = self.dict_kp_seq[sub_part_index]\n                    face_sub_part_point_set = []\n                    for kp_index in range(len(sub_part)):\n                        real_kp_index = sub_part[kp_index]\n                        x, y = pred[real_kp_index * 2], pred[real_kp_index * 2 + 1]\n                        x = int(x * self.display_size[0] // self.rgb888p_size[0])\n                        y = int(y * self.display_size[1] // self.rgb888p_size[1])\n                        face_sub_part_point_set.append((x, y))\n                    # \uff083\uff09\u753b\u4eba\u8138\u4e0d\u540c\u533a\u57df\u7684\u8f6e\u5ed3\n                    if sub_part_index in (9, 6):\n                        color = np.array(self.color_list_for_osd_kp[sub_part_index],dtype = np.uint8)\n                        face_sub_part_point_set = np.array(face_sub_part_point_set)\n                        aidemo.polylines(draw_img_np, face_sub_part_point_set,False,color,5,8,0)\n                    elif sub_part_index == 4:\n                        color = self.color_list_for_osd_kp[sub_part_index]\n                        for kp in face_sub_part_point_set:\n                            x,y = kp[0],kp[1]\n                            draw_img.draw_circle(x,y ,2, color, 1)\n                    else:\n                        color = np.array(self.color_list_for_osd_kp[sub_part_index],dtype = np.uint8)\n                        face_sub_part_point_set = np.array(face_sub_part_point_set)\n                        aidemo.contours(draw_img_np, face_sub_part_point_set,-1,color,2,8)\n            pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[640,360]\n    rgb888p_size = [1920, 1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/examples/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u5173\u952e\u6807\u5fd7\u6a21\u578b\u8def\u5f84\n    face_landmark_kmodel_path="/sdcard/examples/kmodel/face_landmark.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    face_det_input_size=[320,320]\n    face_landmark_input_size=[192,192]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    flm=FaceLandMark(face_det_kmodel_path,face_landmark_kmodel_path,det_input_size=face_det_input_size,landmark_input_size=face_landmark_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,landmark_res=flm.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                flm.draw_result(pl,det_boxes,landmark_res)  # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                             # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        flm.face_det.deinit()\n        flm.face_landmark.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"image-20250423171814319",src:s(69670).Z+"",width:"416",height:"239"})}),"\n",(0,i.jsx)(e.p,{children:"\u200b\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u4e0a\u770b\u5230\u4eba\u8138\u5173\u952e\u90e8\u4f4d\u7684\u7ed3\u679c\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"image-20250423171917279",src:s(1534).Z+"",width:"1484",height:"1024"})})]})}function p(n={}){const{wrapper:e}={...(0,d.a)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(o,{...n})}):o(n)}},69670:(n,e,s)=>{s.d(e,{Z:()=>i});const i=s.p+"assets/images/image-20250423171814319-411480e15f281f7ce43edb04d8412743.png"},1534:(n,e,s)=>{s.d(e,{Z:()=>i});const i=s.p+"assets/images/image-20250423171917279-785163a9c1e2512646884743ce93d581.png"},11151:(n,e,s)=>{s.d(e,{Z:()=>l,a:()=>_});var i=s(67294);const d={},r=i.createContext(d);function _(n){const e=i.useContext(r);return i.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(d):n.components||d:_(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);