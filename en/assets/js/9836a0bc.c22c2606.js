"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9877],{28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>d});var i=s(96540);const _={},t=i.createContext(_);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(_):e.components||_:l(e.components),i.createElement(t.Provider,{value:n},e.children)}},74983:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/image-20250423180320575-52fde2149648cf99455b793090acd3e1.png"},92325:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/image-20250423181019331-811ab24874d7e0ff7a67e1306a1c4938.png"},95750:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>t,metadata:()=>d,toc:()=>a});var i=s(74848),_=s(28453);const t={sidebar_position:4},l="\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b",d={id:"CanaanK230/part14/part3/handKeypointClass",title:"\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b",description:"1.\u5b66\u4e60\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part3/04-handKeypointClass.md",sourceDirName:"CanaanK230/part14/part3",slug:"/CanaanK230/part14/part3/handKeypointClass",permalink:"/en/CanaanK230/part14/part3/handKeypointClass",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part3/04-handKeypointClass.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"canaanK230Sidebar",previous:{title:"\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b",permalink:"/en/CanaanK230/part14/part3/handKeypointDetection"},next:{title:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u8d34\u56fe",permalink:"/en/CanaanK230/part14/part3/dynamicGesture"}},r={},a=[{value:"1.\u5b66\u4e60\u76ee\u7684",id:"1\u5b66\u4e60\u76ee\u7684",level:2},{value:"2.\u793a\u4f8b\u4ee3\u7801",id:"2\u793a\u4f8b\u4ee3\u7801",level:2},{value:"3.\u5b9e\u9a8c\u7ed3\u679c",id:"3\u5b9e\u9a8c\u7ed3\u679c",level:2}];function o(e){const n={code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,_.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b",children:"\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b"}),"\n",(0,i.jsx)(n.h2,{id:"1\u5b66\u4e60\u76ee\u7684",children:"1.\u5b66\u4e60\u76ee\u7684"}),"\n",(0,i.jsx)(n.p,{children:"\u5b66\u4e60\u6444\u50cf\u5934\u7684\u753b\u9762\u8fdb\u884c\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"2\u793a\u4f8b\u4ee3\u7801",children:"2.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0: DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u951a\u6846,\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4f7f\u7528\n        self.anchors=anchors\n        # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides = strides\n        # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        self.nms_option = nms_option\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0c\u5f97\u5230\u624b\u638c\u624b\u52bf\u7ed3\u679c\u548c\u624b\u638c\u5173\u952e\u70b9\u6570\u636e\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            gesture=self.hk_gesture(results_show)\n            results_show[0::2] = results_show[0::2] * (self.display_size[0] / self.rgb888p_size[0])\n            results_show[1::2] = results_show[1::2] * (self.display_size[1] / self.rgb888p_size[1])\n            return results_show,gesture\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n    # \u6c42\u4e24\u4e2avector\u4e4b\u95f4\u7684\u5939\u89d2\n    def hk_vector_2d_angle(self,v1,v2):\n        with ScopedTiming("hk_vector_2d_angle",self.debug_mode > 0):\n            v1_x,v1_y,v2_x,v2_y = v1[0],v1[1],v2[0],v2[1]\n            v1_norm = np.sqrt(v1_x * v1_x+ v1_y * v1_y)\n            v2_norm = np.sqrt(v2_x * v2_x + v2_y * v2_y)\n            dot_product = v1_x * v2_x + v1_y * v2_y\n            cos_angle = dot_product/(v1_norm*v2_norm)\n            angle = np.acos(cos_angle)*180/np.pi\n            return angle\n\n    # \u6839\u636e\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u7ed3\u679c\u5224\u65ad\u624b\u52bf\u7c7b\u522b\n    def hk_gesture(self,results):\n        with ScopedTiming("hk_gesture",self.debug_mode > 0):\n            angle_list = []\n            for i in range(5):\n                angle = self.hk_vector_2d_angle([(results[0]-results[i*8+4]), (results[1]-results[i*8+5])],[(results[i*8+6]-results[i*8+8]),(results[i*8+7]-results[i*8+9])])\n                angle_list.append(angle)\n            thr_angle,thr_angle_thumb,thr_angle_s,gesture_str = 65.,53.,49.,None\n            if 65535. not in angle_list:\n                if (angle_list[0]>thr_angle_thumb)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "fist"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "five"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "gun"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "love"\n                elif (angle_list[0]>5)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "one"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "six"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]>thr_angle):\n                    gesture_str = "three"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "thumbUp"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "yeah"\n            return gesture_str\n\n# \u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\nclass HandKeyPointClass:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        boxes=[]\n        gesture_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u4e8e\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u624b\u638c\u6267\u884c\u5173\u952e\u70b9\u8bc6\u522b\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            self.hand_kp.config_preprocess(det_box)\n            results_show,gesture=self.hand_kp.run(input_np)\n            gesture_res.append((results_show,gesture))\n            boxes.append(det_box)\n        return boxes,gesture_res\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u5173\u952e\u70b9\u3001\u624b\u638c\u68c0\u6d4b\u6846\u548c\u8bc6\u522b\u7ed3\u679c\n    def draw_result(self,pl,dets,gesture_res):\n        pl.osd_img.clear()\n        if len(dets)>0:\n            for k in range(len(dets)):\n                det_box=dets[k]\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                if (h<(0.1*self.rgb888p_size[1])):\n                    continue\n                if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                    continue\n                if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                    continue\n                w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n                h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n                x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n                y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x_det, y_det, w_det, h_det, color=(255, 0, 255, 0), thickness = 2)\n\n                results_show=gesture_res[k][0]\n                for i in range(len(results_show)/2):\n                    pl.osd_img.draw_circle(results_show[i*2], results_show[i*2+1], 1, color=(255, 0, 255, 0),fill=False)\n                for i in range(5):\n                    j = i*8\n                    if i==0:\n                        R = 255; G = 0; B = 0\n                    if i==1:\n                        R = 255; G = 0; B = 255\n                    if i==2:\n                        R = 255; G = 255; B = 0\n                    if i==3:\n                        R = 0; G = 255; B = 0\n                    if i==4:\n                        R = 0; G = 0; B = 255\n                    pl.osd_img.draw_line(results_show[0], results_show[1], results_show[j+2], results_show[j+3], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+2], results_show[j+3], results_show[j+4], results_show[j+5], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+4], results_show[j+5], results_show[j+6], results_show[j+7], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+6], results_show[j+7], results_show[j+8], results_show[j+9], color=(255,R,G,B), thickness = 3)\n\n                gesture_str=gesture_res[k][1]\n                pl.osd_img.draw_string_advanced( x_det , y_det-50,32, " " + str(gesture_str), color=(255,0, 255, 0))\n\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[640,360]\n    rgb888p_size = [1920, 1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/examples/kmodel/hand_det.kmodel"\n    # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/examples/kmodel/handkp_det.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    hkc=HandKeyPointClass(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,gesture_res=hkc.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                hkc.draw_result(pl,det_boxes,gesture_res)   # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                             # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hkc.hand_det.deinit()\n        hkc.hand_kp.deinit()\n        pl.destroy()\n\n\n'})}),"\n",(0,i.jsx)(n.h2,{id:""}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u951a\u6846,\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4f7f\u7528\n        self.anchors=anchors\n        # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides = strides\n        # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        self.nms_option = nms_option\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandKPDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            results_show[0::2] = results_show[0::2] * (self.display_size[0] / self.rgb888p_size[0])\n            results_show[1::2] = results_show[1::2] * (self.display_size[1] / self.rgb888p_size[1])\n            return results_show\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n# \u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\nclass HandKeyPointDet:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # nms\u9009\u9879\n        self.nms_option=nms_option\n        # \u7279\u5f81\u56fe\u5bf9\u4e8e\u8f93\u5165\u7684\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPDetApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        hand_res=[]\n        boxes=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e2a\u624b\u638c\u6267\u884c\u624b\u52bf\u5173\u952e\u70b9\u8bc6\u522b\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            # \u4e22\u5f03\u4e0d\u5408\u7406\u7684\u6846\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            self.hand_kp.config_preprocess(det_box)\n            results_show=self.hand_kp.run(input_np)\n            boxes.append(det_box)\n            hand_res.append(results_show)\n        return boxes,hand_res\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u624b\u638c\u5173\u952e\u70b9\u3001\u68c0\u6d4b\u6846\n    def draw_result(self,pl,dets,hand_res):\n        pl.osd_img.clear()\n        if dets:\n            for k in range(len(dets)):\n                det_box=dets[k]\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n                h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n                x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n                y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x_det, y_det, w_det, h_det, color=(255, 0, 255, 0), thickness = 2)\n\n                results_show=hand_res[k]\n                for i in range(len(results_show)/2):\n                    pl.osd_img.draw_circle(results_show[i*2], results_show[i*2+1], 1, color=(255, 0, 255, 0),fill=False)\n                for i in range(5):\n                    j = i*8\n                    if i==0:\n                        R = 255; G = 0; B = 0\n                    if i==1:\n                        R = 255; G = 0; B = 255\n                    if i==2:\n                        R = 255; G = 255; B = 0\n                    if i==3:\n                        R = 0; G = 255; B = 0\n                    if i==4:\n                        R = 0; G = 0; B = 255\n                    pl.osd_img.draw_line(results_show[0], results_show[1], results_show[j+2], results_show[j+3], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+2], results_show[j+3], results_show[j+4], results_show[j+5], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+4], results_show[j+5], results_show[j+6], results_show[j+7], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+6], results_show[j+7], results_show[j+8], results_show[j+9], color=(255,R,G,B), thickness = 3)\n\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[640,360]\n    rgb888p_size = [1920, 1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/examples/kmodel/hand_det.kmodel"\n    # \u624b\u90e8\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/examples/kmodel/handkp_det.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    hkd=HandKeyPointDet(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,hand_res=hkd.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                hkd.draw_result(pl,det_boxes,hand_res)  # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hkd.hand_det.deinit()\n        hkd.hand_kp.deinit()\n        pl.destroy()\n\n\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3\u5b9e\u9a8c\u7ed3\u679c",children:"3.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250423180320575",src:s(74983).A+"",width:"344",height:"381"})}),"\n",(0,i.jsx)(n.p,{children:"\u200b\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\u53ef\u4ee5\u67e5\u770b\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b\u7ed3\u679c\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250423181019331",src:s(92325).A+"",width:"1443",height:"993"})})]})}function p(e={}){const{wrapper:n}={...(0,_.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}}}]);