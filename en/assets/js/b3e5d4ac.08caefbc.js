"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6282],{55936:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>a,frontMatter:()=>p,metadata:()=>t,toc:()=>o});var i=s(85893),d=s(11151);const p={sidebar_position:2},r="\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",t={id:"CanaanK230/part14/part1/personKeypointDetect",title:"\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part1/02-personKeypointDetect.md",sourceDirName:"CanaanK230/part14/part1",slug:"/CanaanK230/part14/part1/personKeypointDetect",permalink:"/en/CanaanK230/part14/part1/personKeypointDetect",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part1/02-personKeypointDetect.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"canaanK230Sidebar",previous:{title:"\u4eba\u5f62\u68c0\u6d4b",permalink:"/en/CanaanK230/part14/part1/personDetection"},next:{title:"\u8dcc\u5012\u68c0\u6d4b",permalink:"/en/CanaanK230/part14/part1/falldownDetect"}},l={},o=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u6838\u5fc3\u4ee3\u7801",id:"2\u6838\u5fc3\u4ee3\u7801",level:2},{value:"\u56fe\u50cf\u83b7\u53d6",id:"\u56fe\u50cf\u83b7\u53d6",level:3},{value:"\u56fe\u50cf\u9884\u5904\u7406",id:"\u56fe\u50cf\u9884\u5904\u7406",level:3},{value:"\u6a21\u578b\u63a8\u7406",id:"\u6a21\u578b\u63a8\u7406",level:3},{value:"\u540e\u5904\u7406",id:"\u540e\u5904\u7406",level:3},{value:"\u7ed8\u56fe\u663e\u793a",id:"\u7ed8\u56fe\u663e\u793a",level:3},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function _(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,d.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",children:"\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,i.jsx)(n.p,{children:"\u5b66\u4e60\u68c0\u6d4b\u6444\u50cf\u5934\u62cd\u6444\u5230\u7684\u753b\u9762\u4e2d\u7684\u4eba\u4f53\u5173\u952e\u70b9\u5e76\u901a\u8fc7\u753b\u56fe\u63d0\u793a\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"2\u6838\u5fc3\u4ee3\u7801",children:"2.\u6838\u5fc3\u4ee3\u7801"}),"\n",(0,i.jsx)(n.h3,{id:"\u56fe\u50cf\u83b7\u53d6",children:"\u56fe\u50cf\u83b7\u53d6"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\nimg = pl.get_frame()  # \u62ff\u5230\u4e00\u4e2a\u56fe\u50cf\u5e27\uff0c\u4f9b\u540e\u7eed\u6a21\u578b\u4f7f\u7528\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"PipeLine"})," \u7c7b\u83b7\u53d6\u4e00\u5e27\u6765\u81ea\u56fe\u50cf\u4f20\u611f\u5668\uff08Camera\uff09\u7684\u56fe\u50cf\u3002"]}),"\n",(0,i.jsx)(n.p,{children:"\u56fe\u50cf\u7684\u683c\u5f0f\u4e00\u822c\u662f RGB888 planar\uff0c\u5373\u4e09\u4e2a\u901a\u9053\u6570\u636e\u72ec\u7acb\u5b58\u50a8\uff08C, H, W\uff09"}),"\n",(0,i.jsx)(n.h3,{id:"\u56fe\u50cf\u9884\u5904\u7406",children:"\u56fe\u50cf\u9884\u5904\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u7528 ",(0,i.jsx)(n.code,{children:"Ai2D"})," \u5bf9\u56fe\u50cf\u505a ",(0,i.jsx)(n.strong,{children:"Padding + Resize"}),"\uff0c\u8ba9\u5b83\u9002\u914d\u6a21\u578b\u8f93\u5165 ",(0,i.jsx)(n.code,{children:"[320, 320]"}),"\u3002"]}),"\n",(0,i.jsx)(n.p,{children:"\u4fdd\u6301\u7eb5\u6a2a\u6bd4\u7f29\u653e\uff0c\u586b\u5145\u5269\u4f59\u533a\u57df\uff08letterbox padding\uff09\u9632\u6b62\u53d8\u5f62\u3002"}),"\n",(0,i.jsxs)(n.p,{children:["\u8f6c\u6362\u683c\u5f0f\u4e3a ",(0,i.jsx)(n.code,{children:"NCHW"}),"\uff0c\u4e14\u8f6c\u6362\u4e3a uint8 \u7c7b\u578b\u3002"]}),"\n",(0,i.jsx)(n.h3,{id:"\u6a21\u578b\u63a8\u7406",children:"\u6a21\u578b\u63a8\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"                res=person_kp.run(img)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u7ee7\u627f\u81ea ",(0,i.jsx)(n.code,{children:"AIBase"}),"\uff0c\u8fd9\u4e2a\u51fd\u6570\u5185\u90e8\u4f1a\u6267\u884c\uff1a"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u628a RGB \u56fe\u50cf\u9001\u5165 Ai2D \u5904\u7406"}),"\n",(0,i.jsxs)(n.li,{children:["\u63a8\u7406\u6a21\u578b ",(0,i.jsx)(n.code,{children:".kmodel"})]}),"\n",(0,i.jsxs)(n.li,{children:["\u83b7\u53d6\u7ed3\u679c\uff08\u4f8b\u5982\u8f93\u51fa shape \u662f ",(0,i.jsx)(n.code,{children:"[1, 6, 56, 56]"})," \u5f62\u5f0f\uff09"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u540e\u5904\u7406",children:"\u540e\u5904\u7406"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"            results = aidemo.person_kp_postprocess(results[0],[self.rgb888p_size[1],self.rgb888p_size[0]],self.model_input_size,self.confidence_threshold,self.nms_threshold)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u4ece\u6a21\u578b\u8f93\u51fa\u4e2d\u63d0\u53d6\u51fa\u6bcf\u4e2a ",(0,i.jsx)(n.code,{children:"\u5173\u952e\u70b9\u4f4d\u7f6e (x, y)"})," \u548c ",(0,i.jsx)(n.code,{children:"\u7f6e\u4fe1\u5ea6 score"})]}),"\n",(0,i.jsx)(n.p,{children:"\u6267\u884c NMS\uff08\u975e\u6781\u5927\u503c\u6291\u5236\uff09\u4fdd\u7559\u6700\u4f18\u4eba\u4f53\u76ee\u6807"}),"\n",(0,i.jsx)(n.p,{children:"\u5c06 keypoints \u6309\u539f\u56fe\u5750\u6807\u7cfb\u8fdb\u884c\u53cd\u53d8\u6362\uff08\u53bb\u6389 padding \u7f29\u653e\u5f71\u54cd\uff09"}),"\n",(0,i.jsx)(n.p,{children:"\u7ed3\u679c\u683c\u5f0f\u4e3a\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"res[0] = \u6bcf\u4e2a\u4eba\u7684\u8fb9\u754c\u6846\u4fe1\u606f\nres[1] = \u6bcf\u4e2a\u4eba\u7684 17 \u4e2a\u5173\u952e\u70b9\u4fe1\u606f\uff0c\u6bcf\u4e2a\u4e3a [x, y, score]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"\u7ed8\u56fe\u663e\u793a",children:"\u7ed8\u56fe\u663e\u793a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"                person_kp.draw_result(pl,res)\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u904d\u5386\u6bcf\u4e2a\u4eba\u4f53\u76ee\u6807\uff1a"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u753b\u5173\u952e\u70b9\uff08\u5706\u70b9\uff09"}),"\n",(0,i.jsxs)(n.li,{children:["\u6309\u7167 ",(0,i.jsx)(n.code,{children:"SKELETON"})," \u5b9a\u4e49\uff0c\u8fde\u6210\u9aa8\u9abc\u7ebf\uff08\u7ebf\u6bb5\uff09"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\u4f7f\u7528 osd_img \u63a5\u53e3\u7ed8\u5236\uff08On Screen Display\uff09"}),"\n",(0,i.jsx)(n.p,{children:"\u4f8b\u5982\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"pl.osd_img.draw_circle(x, y, \u534a\u5f84, \u989c\u8272, \u7ebf\u5bbd)\npl.osd_img.draw_line(x1, y1, x2, y2, \u989c\u8272, \u7ebf\u5bbd)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming # \u56fe\u50cf\u91c7\u96c6\u548c\u5904\u7406\u7ba1\u9053\u6a21\u5757\nfrom libs.AIBase import AIBase                  # AI \u63a8\u7406\u7684\u57fa\u7840\u7c7b\nfrom libs.AI2D import Ai2d                      # \u56fe\u50cf\u9884\u5904\u7406\u6a21\u5757\nimport os\nimport ujson\nfrom media.media import *                       # \u5a92\u4f53\u6a21\u5757\uff0c\u83b7\u53d6\u56fe\u50cf\u5e27\u7b49\nfrom time import *\nimport nncase_runtime as nn                     # nncase \u63a8\u7406\u8fd0\u884c\u5e93\nimport ulab.numpy as np                         # Ulab \u63d0\u4f9b\u7684 numpy \u652f\u6301\uff08\u8f7b\u91cf\uff09\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo                                 # \u5305\u542b\u5173\u952e\u70b9\u540e\u5904\u7406\u7684\u5de5\u5177\u5305\n\n# \u81ea\u5b9a\u4e49\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u7c7b\nclass PersonKeyPointApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,confidence_threshold=0.2,nms_threshold=0.5,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\u8bbe\u7f6e\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\u8bbe\u7f6e\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        #\u9aa8\u9abc\u4fe1\u606f\n        self.SKELETON = [(16, 14),(14, 12),(17, 15),(15, 13),(12, 13),(6,  12),(7,  13),(6,  7),(6,  8),(7,  9),(8,  10),(9,  11),(2,  3),(1,  2),(1,  3),(2,  4),(3,  5),(4,  6),(5,  7)]\n        #\u80a2\u4f53\u989c\u8272\n        self.LIMB_COLORS = [(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 255, 51,  255),(255, 255, 51,  255),(255, 255, 51,  255),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0)]\n        #\u5173\u952e\u70b9\u989c\u8272\uff0c\u517117\u4e2a\n        self.KPS_COLORS = [(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255)]\n\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u6570\u636e\u683c\u5f0f\u4e3a NCHW\uff0c\u6570\u636e\u7c7b\u578b\u4e3a uint8\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684person_kp_postprocess\u63a5\u53e3\n            results = aidemo.person_kp_postprocess(results[0],[self.rgb888p_size[1],self.rgb888p_size[0]],self.model_input_size,self.confidence_threshold,self.nms_threshold)\n            return results\n\n    #\u7ed8\u5236\u7ed3\u679c\uff0c\u7ed8\u5236\u4eba\u4f53\u5173\u952e\u70b9\n    def draw_result(self,pl,res):\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            if res[0]:\n                pl.osd_img.clear()\n                kpses = res[1]\n                for i in range(len(res[0])):\n                    for k in range(17+2):\n                        if (k < 17):\n                            kps_x,kps_y,kps_s = round(kpses[i][k][0]),round(kpses[i][k][1]),kpses[i][k][2]\n                            kps_x1 = int(float(kps_x) * self.display_size[0] // self.rgb888p_size[0])\n                            kps_y1 = int(float(kps_y) * self.display_size[1] // self.rgb888p_size[1])\n                            if (kps_s > 0):\n                                pl.osd_img.draw_circle(kps_x1,kps_y1,5,self.KPS_COLORS[k],4)\n                        ske = self.SKELETON[k]\n                        pos1_x,pos1_y= round(kpses[i][ske[0]-1][0]),round(kpses[i][ske[0]-1][1])\n                        pos1_x_ = int(float(pos1_x) * self.display_size[0] // self.rgb888p_size[0])\n                        pos1_y_ = int(float(pos1_y) * self.display_size[1] // self.rgb888p_size[1])\n\n                        pos2_x,pos2_y = round(kpses[i][(ske[1] -1)][0]),round(kpses[i][(ske[1] -1)][1])\n                        pos2_x_ = int(float(pos2_x) * self.display_size[0] // self.rgb888p_size[0])\n                        pos2_y_ = int(float(pos2_y) * self.display_size[1] // self.rgb888p_size[1])\n\n                        pos1_s,pos2_s = kpses[i][(ske[0] -1)][2],kpses[i][(ske[1] -1)][2]\n                        if (pos1_s > 0.0 and pos2_s >0.0):\n                            pl.osd_img.draw_line(pos1_x_,pos1_y_,pos2_x_,pos2_y_,self.LIMB_COLORS[k],4)\n                    gc.collect()\n            else:\n                pl.osd_img.clear()\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw - 0.1))\n        return  top, bottom, left, right\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[640,360]\n    rgb888p_size = [1920, 1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/examples/kmodel/yolov8n-pose.kmodel"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.5\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u5b9e\u4f8b\n    person_kp=PersonKeyPointApp(kmodel_path,model_input_size=[320,320],confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    person_kp.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=person_kp.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                person_kp.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        person_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"person_239",src:s(62082).Z+"",width:"640",height:"480"})}),"\n",(0,i.jsx)(n.p,{children:"\u200b\t\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u4e0a\u770b\u5230\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u7ed3\u679c\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250423165614605",src:s(78709).Z+"",width:"954",height:"638"})})]})}function a(e={}){const{wrapper:n}={...(0,d.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(_,{...e})}):_(e)}},78709:(e,n,s)=>{s.d(n,{Z:()=>i});const i=s.p+"assets/images/image-20250423165614605-df780f2360df3069cb35564c3bc22374.png"},62082:(e,n,s)=>{s.d(n,{Z:()=>i});const i=s.p+"assets/images/person_239-1cc4fc06efbfaf8124404503cdd5a3fd.png"},11151:(e,n,s)=>{s.d(n,{Z:()=>t,a:()=>r});var i=s(67294);const d={},p=i.createContext(d);function r(e){const n=i.useContext(p);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:r(e.components),i.createElement(p.Provider,{value:n},e.children)}}}]);