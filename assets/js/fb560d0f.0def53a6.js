"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[216],{68897:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>p,contentTitle:()=>i,default:()=>d,frontMatter:()=>t,metadata:()=>a,toc:()=>o});var _=s(85893),r=s(11151);const t={sidebar_position:7},i="\u5b9e\u65f6\u8ddf\u8e2a",a={id:"CanaanK230/part14/Nanotracker",title:"\u5b9e\u65f6\u8ddf\u8e2a",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/02-Nanotracker.md",sourceDirName:"CanaanK230/part14",slug:"/CanaanK230/part14/Nanotracker",permalink:"/CanaanK230/part14/Nanotracker",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/02-Nanotracker.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"canaanK230Sidebar",previous:{title:"YOLOV8\u76ee\u6807\u5206\u5272",permalink:"/CanaanK230/part14/part6/segmentYolov8n"},next:{title:"\u81ea\u5206\u7c7b\u5b66\u4e60",permalink:"/CanaanK230/part14/selfLearning"}},p={},o=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u793a\u4f8b\u4ee3\u7801",id:"2\u793a\u4f8b\u4ee3\u7801",level:2},{value:"3.\u5b9e\u9a8c\u7ed3\u679c",id:"3\u5b9e\u9a8c\u7ed3\u679c",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,r.a)(),...e.components};return(0,_.jsxs)(_.Fragment,{children:[(0,_.jsx)(n.h1,{id:"\u5b9e\u65f6\u8ddf\u8e2a",children:"\u5b9e\u65f6\u8ddf\u8e2a"}),"\n",(0,_.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,_.jsx)(n.p,{children:"\u4ece\u56fe\u50cf\u4e2d\u91c7\u96c6\u611f\u5174\u8da3\u7269\u4f53\u7684\u7279\u5f81\u5e76\u5bf9\u5176\u8fdb\u884c\u8ffd\u8e2a\u3002"}),"\n",(0,_.jsx)(n.h2,{id:"2\u793a\u4f8b\u4ee3\u7801",children:"2.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,_.jsx)(n.pre,{children:(0,_.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0: DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom random import randint\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u8ddf\u8e2a\u6a21\u7248\u4efb\u52a1\u7c7b\nclass TrackCropApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,ratio_src_crop,center_xy_wh,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u8ddf\u8e2a\u6a21\u677f\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        #src\u6a21\u578b\u548ccrop\u6a21\u578b\u8f93\u5165\u6bd4\u503c\n        self.ratio_src_crop = ratio_src_crop\n        self.center_xy_wh=center_xy_wh\n        # padding\u548ccrop\u53c2\u6570\n        self.pad_crop_params=[]\n        # \u6ce8\u610f\uff1aai2d\u8bbe\u7f6e\u591a\u4e2a\u9884\u5904\u7406\u65f6\u6267\u884c\u7684\u987a\u5e8f\u4e3a\uff1acrop->shift->resize/affine->pad\uff0c\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u914d\u7f6e\u591a\u4e2aai2d\u5bf9\u8c61;\n        # \u5982\u4e0b\u6a21\u578b\u9884\u5904\u7406\u8981\u5148\u505aresize+padding\u518d\u505aresize+crop\uff0c\u56e0\u6b64\u8981\u914d\u7f6e\u4e24\u4e2aAi2d\u5bf9\u8c61\n        self.ai2d_pad=Ai2d(debug_mode)\n        self.ai2d_pad.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.ai2d_crop=Ai2d(debug_mode)\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.need_pad=False\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u3001pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            self.pad_crop_params= self.get_padding_crop_param()\n            # \u5982\u679c\u9700\u8981padding,\u914d\u7f6epadding\u90e8\u5206\uff0c\u5426\u5219\u53ea\u8d70crop\n            if (self.pad_crop_params[0] != 0 or self.pad_crop_params[1] != 0 or self.pad_crop_params[2] != 0 or self.pad_crop_params[3] != 0):\n                self.need_pad=True\n                self.ai2d_pad.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_pad.pad([0, 0, 0, 0, self.pad_crop_params[0], self.pad_crop_params[1], self.pad_crop_params[2], self.pad_crop_params[3]], 0, [114, 114, 114])\n                output_size=[self.rgb888p_size[0]+self.pad_crop_params[2]+self.pad_crop_params[3],self.rgb888p_size[1]+self.pad_crop_params[0]+self.pad_crop_params[1]]\n                self.ai2d_pad.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,output_size[1],output_size[0]])\n\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(self.pad_crop_params[4]),int(self.pad_crop_params[6]),int(self.pad_crop_params[5]-self.pad_crop_params[4]+1),int(self.pad_crop_params[7]-self.pad_crop_params[6]+1))\n                self.ai2d_crop.build([1,3,output_size[1],output_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            else:\n                self.need_pad=False\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(self.center_xy_wh[0]-self.pad_crop_params[8]/2.0),int(self.center_xy_wh[1]-self.pad_crop_params[8]/2.0),int(self.pad_crop_params[8]),int(self.pad_crop_params[8]))\n                self.ai2d_crop.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u91cd\u5199\u9884\u5904\u7406\u51fd\u6570preprocess\uff0c\u56e0\u4e3a\u8be5\u90e8\u5206\u4e0d\u662f\u5355\u7eaf\u7684\u8d70\u4e00\u4e2aai2d\u505a\u9884\u5904\u7406\uff0c\u6240\u4ee5\u8be5\u51fd\u6570\u9700\u8981\u91cd\u5199\n    def preprocess(self,input_np):\n        if self.need_pad:\n            pad_output=self.ai2d_pad.run(input_np).to_numpy()\n            return [self.ai2d_crop.run(pad_output)]\n        else:\n            return [self.ai2d_crop.run(input_np)]\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0]\n\n    # \u8ba1\u7b97padding\u548ccrop\u53c2\u6570\n    def get_padding_crop_param(self):\n        s_z = round(np.sqrt((self.center_xy_wh[2] + self.CONTEXT_AMOUNT * (self.center_xy_wh[2] + self.center_xy_wh[3])) * (self.center_xy_wh[3] + self.CONTEXT_AMOUNT * (self.center_xy_wh[2] + self.center_xy_wh[3]))))\n        c = (s_z + 1) / 2\n        context_xmin = np.floor(self.center_xy_wh[0] - c + 0.5)\n        context_xmax = int(context_xmin + s_z - 1)\n        context_ymin = np.floor(self.center_xy_wh[1] - c + 0.5)\n        context_ymax = int(context_ymin + s_z - 1)\n        left_pad = int(max(0, -context_xmin))\n        top_pad = int(max(0, -context_ymin))\n        right_pad = int(max(0, int(context_xmax - self.rgb888p_size[0] + 1)))\n        bottom_pad = int(max(0, int(context_ymax - self.rgb888p_size[1] + 1)))\n        context_xmin = context_xmin + left_pad\n        context_xmax = context_xmax + left_pad\n        context_ymin = context_ymin + top_pad\n        context_ymax = context_ymax + top_pad\n        return [top_pad,bottom_pad,left_pad,right_pad,context_xmin,context_xmax,context_ymin,context_ymax,s_z]\n\n    #\u91cd\u5199deinit\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.ai2d_pad\n            del self.ai2d_crop\n            super().deinit()\n\n# \u81ea\u5b9a\u4e49\u8ddf\u8e2a\u5b9e\u65f6\u4efb\u52a1\u7c7b\nclass TrackSrcApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,ratio_src_crop,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # padding\u548ccrop\u53c2\u6570\u5217\u8868\n        self.pad_crop_params=[]\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        # src\u548ccrop\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\u6bd4\u4f8b\n        self.ratio_src_crop = ratio_src_crop\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6ce8\u610f\uff1aai2d\u8bbe\u7f6e\u591a\u4e2a\u9884\u5904\u7406\u65f6\u6267\u884c\u7684\u987a\u5e8f\u4e3a\uff1acrop->shift->resize/affine->pad\uff0c\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u914d\u7f6e\u591a\u4e2aai2d\u5bf9\u8c61;\n        # \u5982\u4e0b\u6a21\u578b\u9884\u5904\u7406\u8981\u5148\u505aresize+padding\u518d\u505aresize+crop\uff0c\u56e0\u6b64\u8981\u914d\u7f6e\u4e24\u4e2aAi2d\u5bf9\u8c61\n        self.ai2d_pad=Ai2d(debug_mode)\n        self.ai2d_pad.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.ai2d_crop=Ai2d(debug_mode)\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.need_pad=False\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u3001pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,center_xy_wh,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            self.pad_crop_params= self.get_padding_crop_param(center_xy_wh)\n            # \u5982\u679c\u9700\u8981padding,\u914d\u7f6epadding\u90e8\u5206\uff0c\u5426\u5219\u53ea\u8d70crop\n            if (self.pad_crop_params[0] != 0 or self.pad_crop_params[1] != 0 or self.pad_crop_params[2] != 0 or self.pad_crop_params[3] != 0):\n                self.need_pad=True\n                self.ai2d_pad.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_pad.pad([0, 0, 0, 0, self.pad_crop_params[0], self.pad_crop_params[1], self.pad_crop_params[2], self.pad_crop_params[3]], 0, [114, 114, 114])\n                output_size=[self.rgb888p_size[0]+self.pad_crop_params[2]+self.pad_crop_params[3],self.rgb888p_size[1]+self.pad_crop_params[0]+self.pad_crop_params[1]]\n\n                self.ai2d_pad.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,output_size[1],output_size[0]])\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(self.pad_crop_params[4]),int(self.pad_crop_params[6]),int(self.pad_crop_params[5]-self.pad_crop_params[4]+1),int(self.pad_crop_params[7]-self.pad_crop_params[6]+1))\n                self.ai2d_crop.build([1,3,output_size[1],output_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            else:\n                self.need_pad=False\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(center_xy_wh[0]-self.pad_crop_params[8]/2.0),int(center_xy_wh[1]-self.pad_crop_params[8]/2.0),int(self.pad_crop_params[8]),int(self.pad_crop_params[8]))\n                self.ai2d_crop.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u91cd\u5199\u9884\u5904\u7406\u51fd\u6570preprocess\uff0c\u56e0\u4e3a\u8be5\u90e8\u5206\u4e0d\u662f\u5355\u7eaf\u7684\u8d70\u4e00\u4e2aai2d\u505a\u9884\u5904\u7406\uff0c\u6240\u4ee5\u8be5\u51fd\u6570\u9700\u8981\u91cd\u5199\n    def preprocess(self,input_np):\n        with ScopedTiming("preprocess",self.debug_mode>0):\n            if self.need_pad:\n                pad_output=self.ai2d_pad.run(input_np).to_numpy()\n                return [self.ai2d_crop.run(pad_output)]\n            else:\n                return [self.ai2d_crop.run(input_np)]\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0]\n\n    # \u8ba1\u7b97padding\u548ccrop\u53c2\u6570\n    def get_padding_crop_param(self,center_xy_wh):\n        s_z = round(np.sqrt((center_xy_wh[2] + self.CONTEXT_AMOUNT * (center_xy_wh[2] + center_xy_wh[3])) * (center_xy_wh[3] + self.CONTEXT_AMOUNT * (center_xy_wh[2] + center_xy_wh[3])))) * self.ratio_src_crop\n        c = (s_z + 1) / 2\n        context_xmin = np.floor(center_xy_wh[0] - c + 0.5)\n        context_xmax = int(context_xmin + s_z - 1)\n        context_ymin = np.floor(center_xy_wh[1] - c + 0.5)\n        context_ymax = int(context_ymin + s_z - 1)\n        left_pad = int(max(0, -context_xmin))\n        top_pad = int(max(0, -context_ymin))\n        right_pad = int(max(0, int(context_xmax - self.rgb888p_size[0] + 1)))\n        bottom_pad = int(max(0, int(context_ymax - self.rgb888p_size[1] + 1)))\n        context_xmin = context_xmin + left_pad\n        context_xmax = context_xmax + left_pad\n        context_ymin = context_ymin + top_pad\n        context_ymax = context_ymax + top_pad\n        return [top_pad,bottom_pad,left_pad,right_pad,context_xmin,context_xmax,context_ymin,context_ymax,s_z]\n\n    # \u91cd\u5199deinit\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.ai2d_pad\n            del self.ai2d_crop\n            super().deinit()\n\n\nclass TrackerApp(AIBase):\n    def __init__(self,kmodel_path,crop_input_size,thresh,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # crop\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\n        self.crop_input_size=crop_input_size\n        # \u8ddf\u8e2a\u6846\u9608\u503c\n        self.thresh=thresh\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            pass\n\n    # \u91cd\u5199run\u51fd\u6570\uff0c\u56e0\u4e3a\u6ca1\u6709\u9884\u5904\u7406\u8fc7\u7a0b\uff0c\u6240\u4ee5\u539f\u6765run\u64cd\u4f5c\u4e2d\u5305\u542b\u7684preprocess->inference->postprocess\u4e0d\u5408\u9002\uff0c\u8fd9\u91cc\u53ea\u5305\u542binference->postprocess\n    def run(self,input_np_1,input_np_2,center_xy_wh):\n        input_tensors=[]\n        input_tensors.append(nn.from_numpy(input_np_1))\n        input_tensors.append(nn.from_numpy(input_np_2))\n        results=self.inference(input_tensors)\n        return self.postprocess(results,center_xy_wh)\n\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868,\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u7684nanotracker_postprocess\u5217\u8868\n    def postprocess(self,results,center_xy_wh):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            det = aidemo.nanotracker_postprocess(results[0],results[1],[self.rgb888p_size[1],self.rgb888p_size[0]],self.thresh,center_xy_wh,self.crop_input_size[0],self.CONTEXT_AMOUNT)\n            return det\n\nclass NanoTracker:\n    def __init__(self,track_crop_kmodel,track_src_kmodel,tracker_kmodel,crop_input_size,src_input_size,threshold=0.25,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u8ddf\u8e2a\u6a21\u7248\u6a21\u578b\u8def\u5f84\n        self.track_crop_kmodel=track_crop_kmodel\n        # \u8ddf\u8e2a\u5b9e\u65f6\u6a21\u578b\u8def\u5f84\n        self.track_src_kmodel=track_src_kmodel\n        # \u8ddf\u8e2a\u6a21\u578b\u8def\u5f84\n        self.tracker_kmodel=tracker_kmodel\n        # \u8ddf\u8e2a\u6a21\u7248\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.crop_input_size=crop_input_size\n        # \u8ddf\u8e2a\u5b9e\u65f6\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.src_input_size=src_input_size\n        self.threshold=threshold\n\n        self.CONTEXT_AMOUNT=0.5       # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.ratio_src_crop = 0.0     # src\u6a21\u578b\u548ccrop\u6a21\u578b\u8f93\u5165\u6bd4\u503c\n        self.track_x1 = float(600)    # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846\u5de6\u4e0a\u89d2\u70b9x\n        self.track_y1 = float(300)    # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846\u5de6\u4e0a\u89d2\u70b9y\n        self.track_w = float(100)     # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846w\n        self.track_h = float(100)     # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846h\n        self.draw_mean=[]             # \u521d\u59cb\u76ee\u6807\u6846\u4f4d\u7f6e\u5217\u8868\n        self.center_xy_wh = []\n        self.track_boxes = []\n        self.center_xy_wh_tmp = []\n        self.track_boxes_tmp=[]\n        self.crop_output=None\n        self.src_output=None\n        # \u8ddf\u8e2a\u6846\u521d\u59cb\u5316\u65f6\u95f4\n        self.seconds = 8\n        self.endtime = time.time() + self.seconds\n        self.enter_init = True\n\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.init_param()\n\n        self.track_crop=TrackCropApp(self.track_crop_kmodel,model_input_size=self.crop_input_size,ratio_src_crop=self.ratio_src_crop,center_xy_wh=self.center_xy_wh,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.track_src=TrackSrcApp(self.track_src_kmodel,model_input_size=self.src_input_size,ratio_src_crop=self.ratio_src_crop,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.tracker=TrackerApp(self.tracker_kmodel,crop_input_size=self.crop_input_size,thresh=self.threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.track_crop.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5728\u521d\u59cb\u5316\u65f6\u95f4\u5185\uff0ccrop\u6a21\u7248\u90e8\u5206\u7684\u5230\u8ddf\u8e2a\u6a21\u7248\u7279\u5f81\uff0c\u5426\u5219\uff0c\u5bf9\u5f53\u524d\u5e27\u8fdb\u884csrc\u63a8\u7406\u5f97\u5230\u7279\u5f81\u5e76\u4f7f\u7528tracker\u5bf9\u4e24\u4e2a\u7279\u5f81\u63a8\u7406\uff0c\u5f97\u5230\u8ddf\u8e2a\u6846\u7684\u5750\u6807\n        nowtime = time.time()\n        if (self.enter_init and nowtime <= self.endtime):\n            print("\u5012\u8ba1\u65f6: " + str(self.endtime - nowtime) + " \u79d2")\n            self.crop_output=self.track_crop.run(input_np)\n            time.sleep(1)\n            return self.draw_mean\n        else:\n            self.track_src.config_preprocess(self.center_xy_wh)\n            self.src_output=self.track_src.run(input_np)\n            det=self.tracker.run(self.crop_output,self.src_output,self.center_xy_wh)\n            return det\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u8ddf\u8e2a\u6846\u4f4d\u7f6e\n    def draw_result(self,pl,box):\n        pl.osd_img.clear()\n        if self.enter_init:\n            pl.osd_img.draw_rectangle(box[0],box[1],box[2],box[3],color=(255, 0, 255, 0),thickness = 4)\n            if (time.time() > self.endtime):\n                self.enter_init = False\n        else:\n            self.track_boxes = box[0]\n            self.center_xy_wh = box[1]\n            track_bool = True\n            if (len(self.track_boxes) != 0):\n                track_bool = self.track_boxes[0] > 10 and self.track_boxes[1] > 10 and self.track_boxes[0] + self.track_boxes[2] < self.rgb888p_size[0] - 10 and self.track_boxes[1] + self.track_boxes[3] < self.rgb888p_size[1] - 10\n            else:\n                track_bool = False\n\n            if (len(self.center_xy_wh) != 0):\n                track_bool = track_bool and self.center_xy_wh[2] * self.center_xy_wh[3] < 40000\n            else:\n                track_bool = False\n            if (track_bool):\n                self.center_xy_wh_tmp = self.center_xy_wh\n                self.track_boxes_tmp = self.track_boxes\n                x1 = int(float(self.track_boxes[0]) * self.display_size[0] / self.rgb888p_size[0])\n                y1 = int(float(self.track_boxes[1]) * self.display_size[1] / self.rgb888p_size[1])\n                w = int(float(self.track_boxes[2]) * self.display_size[0] / self.rgb888p_size[0])\n                h = int(float(self.track_boxes[3]) * self.display_size[1] / self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x1, y1, w, h, color=(255, 255, 0, 0),thickness = 4)\n            else:\n                self.center_xy_wh = self.center_xy_wh_tmp\n                self.track_boxes = self.track_boxes_tmp\n                x1 = int(float(self.track_boxes[0]) * self.display_size[0] / self.rgb888p_size[0])\n                y1 = int(float(self.track_boxes[1]) * self.display_size[1] / self.rgb888p_size[1])\n                w = int(float(self.track_boxes[2]) * self.display_size[0] / self.rgb888p_size[0])\n                h = int(float(self.track_boxes[3]) * self.display_size[1] / self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x1, y1, w, h, color=(255, 255, 0, 0),thickness = 4)\n                pl.osd_img.draw_string_advanced( x1 , y1-50,32, "\u8bf7\u8fdc\u79bb\u6444\u50cf\u5934\uff0c\u4fdd\u6301\u8ddf\u8e2a\u7269\u4f53\u5927\u5c0f\u57fa\u672c\u4e00\u81f4!" , color=(255, 255 ,0 , 0))\n                pl.osd_img.draw_string_advanced( x1 , y1-100,32, "\u8bf7\u9760\u8fd1\u4e2d\u5fc3!" , color=(255, 255 ,0 , 0))\n\n    # crop\u53c2\u6570\u521d\u59cb\u5316\n    def init_param(self):\n        self.ratio_src_crop = float(self.src_input_size[0])/float(self.crop_input_size[0])\n        print(self.ratio_src_crop)\n        if (self.track_x1 < 50 or self.track_y1 < 50 or self.track_x1+self.track_w >= self.rgb888p_size[0]-50 or self.track_y1+self.track_h >= self.rgb888p_size[1]-50):\n                    print("**\u526a\u5207\u8303\u56f4\u8d85\u51fa\u56fe\u50cf\u8303\u56f4**")\n        else:\n            track_mean_x = self.track_x1 + self.track_w / 2.0\n            track_mean_y = self.track_y1 + self.track_h / 2.0\n            draw_mean_w = int(self.track_w / self.rgb888p_size[0] * self.display_size[0])\n            draw_mean_h = int(self.track_h / self.rgb888p_size[1] * self.display_size[1])\n            draw_mean_x = int(track_mean_x / self.rgb888p_size[0] * self.display_size[0] - draw_mean_w / 2.0)\n            draw_mean_y = int(track_mean_y / self.rgb888p_size[1] * self.display_size[1] - draw_mean_h / 2.0)\n            self.draw_mean=[draw_mean_x,draw_mean_y,draw_mean_w,draw_mean_h]\n            self.center_xy_wh = [track_mean_x,track_mean_y,self.track_w,self.track_h]\n            self.center_xy_wh_tmp=[track_mean_x,track_mean_y,self.track_w,self.track_h]\n\n            self.track_boxes = [self.track_x1,self.track_y1,self.track_w,self.track_h,1]\n            self.track_boxes_tmp=np.array([self.track_x1,self.track_y1,self.track_w,self.track_h,1])\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    rgb888p_size=[1280,720]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8ddf\u8e2a\u6a21\u677f\u6a21\u578b\u8def\u5f84\n    track_crop_kmodel_path="/sdcard/examples/kmodel/cropped_test127.kmodel"\n    # \u8ddf\u8e2a\u5b9e\u65f6\u6a21\u578b\u8def\u5f84\n    track_src_kmodel_path="/sdcard/examples/kmodel/nanotrack_backbone_sim.kmodel"\n    # \u8ddf\u8e2a\u6a21\u578b\u8def\u5f84\n    tracker_kmodel_path="/sdcard/examples/kmodel/nanotracker_head_calib_k230.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    track_crop_input_size=[127,127]\n    track_src_input_size=[255,255]\n    threshold=0.1\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    track=NanoTracker(track_crop_kmodel_path,track_src_kmodel_path,tracker_kmodel_path,crop_input_size=track_crop_input_size,src_input_size=track_src_input_size,threshold=threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()              # \u83b7\u53d6\u5f53\u524d\u5e27\n                output=track.run(img)           # \u63a8\u7406\u5f53\u524d\u5e27\n                track.draw_result(pl,output)    # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                 # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        track.track_crop.deinit()\n        track.track_src.deinit()\n        track.tracker.deinit()\n        pl.destroy()\n'})}),"\n",(0,_.jsx)(n.h2,{id:"3\u5b9e\u9a8c\u7ed3\u679c",children:"3.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,_.jsx)(n.p,{children:(0,_.jsx)(n.img,{alt:"image-20250423183824019",src:s(33341).Z+"",width:"616",height:"611"})}),"\n",(0,_.jsx)(n.p,{children:"\u70b9\u51fb\u8fd0\u884c\u4e4b\u540e\uff0c\u6211\u4eec\u5c06\u6f14\u793a\u5bf9\u82f9\u679c\u8fdb\u884c\u8ddf\u8e2a\uff0c\u5c06\u82f9\u679c\u653e\u8fdb\u7eff\u8272\u77e9\u5f62\u6846\u3002"}),"\n",(0,_.jsx)(n.p,{children:(0,_.jsx)(n.img,{alt:"image-20250423184135201",src:s(28812).Z+"",width:"1039",height:"761"})}),"\n",(0,_.jsx)(n.p,{children:"\u7b49\u5f85\u77e9\u5f62\u6846\u53d8\u6210\u7ea2\u8272\u540e\u8bf4\u660e\u5b66\u4e60\u6210\u529f\u5e76\u5f00\u59cb\u8ffd\u8e2a\uff0c\u63a5\u4e0b\u6765\u53ef\u4ee5\u79fb\u52a8\u6444\u50cf\u5934\uff0c\u53ef\u4ee5\u53d1\u73b0\u4e00\u76f4\u8ddf\u8e2a\u82f9\u679c\u3002"}),"\n",(0,_.jsx)(n.p,{children:(0,_.jsx)(n.img,{alt:"image-20250423184226374",src:s(87595).Z+"",width:"1204",height:"871"})})]})}function d(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,_.jsx)(n,{...e,children:(0,_.jsx)(l,{...e})}):l(e)}},33341:(e,n,s)=>{s.d(n,{Z:()=>_});const _=s.p+"assets/images/image-20250423183824019-2115130d3593ca190494d11f54a9ca3e.png"},28812:(e,n,s)=>{s.d(n,{Z:()=>_});const _=s.p+"assets/images/image-20250423184135201-ed9acbf91040f0ca0b2c6e187ca3fd7c.png"},87595:(e,n,s)=>{s.d(n,{Z:()=>_});const _=s.p+"assets/images/image-20250423184226374-2d8d67ae0b812e0e6d381ccbf87fba06.png"},11151:(e,n,s)=>{s.d(n,{Z:()=>a,a:()=>i});var _=s(67294);const r={},t=_.createContext(r);function i(e){const n=_.useContext(t);return _.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),_.createElement(t.Provider,{value:n},e.children)}}}]);