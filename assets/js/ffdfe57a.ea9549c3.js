"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5061],{12232:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>t,default:()=>c,frontMatter:()=>l,metadata:()=>r,toc:()=>p});var i=s(85893),d=s(11151);const l={sidebar_position:1},t="\u4eba\u5f62\u68c0\u6d4b",r={id:"CanaanK230/part14/part1/personDetection",title:"\u4eba\u5f62\u68c0\u6d4b",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part1/01-personDetection.md",sourceDirName:"CanaanK230/part14/part1",slug:"/CanaanK230/part14/part1/personDetection",permalink:"/CanaanK230/part14/part1/personDetection",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part1/01-personDetection.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"canaanK230Sidebar",previous:{title:"\u4eba\u4f53\u76f8\u5173",permalink:"/category/\u4eba\u4f53\u76f8\u5173"},next:{title:"\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",permalink:"/CanaanK230/part14/part1/personKeypointDetect"}},o={},p=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u4ee3\u7801\u89e3\u6790",id:"2\u4ee3\u7801\u89e3\u6790",level:2},{value:"\u6838\u5fc3\u7c7b\u89e3\u6790",id:"\u6838\u5fc3\u7c7b\u89e3\u6790",level:3},{value:"\u56fe\u50cf\u91c7\u96c6\u4e0e\u663e\u793a",id:"\u56fe\u50cf\u91c7\u96c6\u4e0e\u663e\u793a",level:3},{value:"\u56fe\u50cf\u5904\u7406\u6d41\u7a0b",id:"\u56fe\u50cf\u5904\u7406\u6d41\u7a0b",level:3},{value:"\u7ed8\u5236\u7ed3\u679c",id:"\u7ed8\u5236\u7ed3\u679c",level:3},{value:"\u4e3b\u5faa\u73af",id:"\u4e3b\u5faa\u73af",level:3},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"\u4eba\u5f62\u68c0\u6d4b",children:"\u4eba\u5f62\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,i.jsx)(n.p,{children:"\u57fa\u4e8eYOLOv5n \u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u4eba\u4f53\u7684\u5b9e\u65f6\u68c0\u6d4b\uff0c\u56fe\u50cf\u83b7\u53d6\u3001AI \u9884\u5904\u7406\u3001\u63a8\u7406\u3001\u540e\u5904\u7406\u548c\u663e\u793a\u5b8c\u6574\u4e32\u8054\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"2\u4ee3\u7801\u89e3\u6790",children:"2.\u4ee3\u7801\u89e3\u6790"}),"\n",(0,i.jsx)(n.h3,{id:"\u6838\u5fc3\u7c7b\u89e3\u6790",children:"\u6838\u5fc3\u7c7b\u89e3\u6790"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"PersonDetectionApp"}),"\uff1a\u81ea\u5b9a\u4e49\u4eba\u4f53\u68c0\u6d4b\u7c7b"]}),"\n",(0,i.jsxs)(n.p,{children:["\u7ee7\u627f\u81ea ",(0,i.jsx)(n.code,{children:"AIBase"}),"\uff0c\u5c01\u88c5\u4e86\u4ee5\u4e0b\u529f\u80fd\uff1a"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u6a21\u578b\u521d\u59cb\u5316\u3001\u52a0\u8f7d"}),"\n",(0,i.jsx)(n.li,{children:"Ai2d \u56fe\u50cf\u9884\u5904\u7406\u914d\u7f6e\uff08pad + resize\uff09"}),"\n",(0,i.jsxs)(n.li,{children:["YOLO \u8f93\u51fa\u7684\u540e\u5904\u7406\uff08\u8c03\u7528 ",(0,i.jsx)(n.code,{children:"aicube.anchorbasedet_post_process"}),"\uff09"]}),"\n",(0,i.jsx)(n.li,{children:"\u7ed8\u5236\u68c0\u6d4b\u6846\u53ca\u6807\u7b7e\u5230\u663e\u793a\u5c4f"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'person_det = PersonDetectionApp(\n    kmodel_path="/sdcard/examples/kmodel/person_detect_yolov5n.kmodel",\n    model_input_size=[640,640],\n    labels=["person"],\n    anchors=[...],\n    ...\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"\u56fe\u50cf\u91c7\u96c6\u4e0e\u663e\u793a",children:"\u56fe\u50cf\u91c7\u96c6\u4e0e\u663e\u793a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'pl = PipeLine(rgb888p_size=[1920, 1080], display_size=[800, 480], display_mode="lcd")\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u5c01\u88c5\u4e86\u56fe\u50cf\u4f20\u611f\u5668\u7684\u521d\u59cb\u5316\u3001\u5e27\u83b7\u53d6\u3001OSD \u56fe\u5c42\u7ed8\u5236\u3001\u56fe\u50cf\u663e\u793a\u7b49\u529f\u80fd\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u56fe\u50cf\u5904\u7406\u6d41\u7a0b",children:"\u56fe\u50cf\u5904\u7406\u6d41\u7a0b"}),"\n",(0,i.jsxs)(n.p,{children:["1.\u9884\u5904\u7406\uff1a",(0,i.jsx)(n.code,{children:"config_preprocess()"})]}),"\n",(0,i.jsxs)(n.p,{children:["\u901a\u8fc7 ",(0,i.jsx)(n.code,{children:"Ai2d"})," \u7edf\u4e00\u8bbe\u7f6e\u8f93\u5165\u56fe\u50cf\u683c\u5f0f\u3001\u7c7b\u578b\u548c\u5c3a\u5bf8\uff0c\u5e38\u89c1\u64cd\u4f5c\u5305\u62ec\uff1a"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pad"}),"\uff1a\u586b\u5145\u8fb9\u7f18\uff0c\u9632\u6b62\u56fe\u50cf\u5931\u771f"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resize"}),"\uff1a\u7f29\u653e\u5230\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\uff08\u5982 640x640\uff09"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"self.ai2d.pad([...])\nself.ai2d.resize(...)\nself.ai2d.build(...)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["2.\u63a8\u7406\uff1a",(0,i.jsx)(n.code,{children:"run()"}),"\uff08\u7ee7\u627f\u81ea AIBase\uff09"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"res = person_det.run(img)\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u4f1a\u5185\u90e8\u8c03\u7528\uff1a"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"self.preprocess(img)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"self.interpreter.run(...)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"self.postprocess(results)"})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["3.\u540e\u5904\u7406\uff1a",(0,i.jsx)(n.code,{children:"postprocess()"})]}),"\n",(0,i.jsxs)(n.p,{children:["\u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"aicube.anchorbasedet_post_process()"})," \u5b9e\u73b0 anchor-based \u68c0\u6d4b\u7ed3\u679c\u8f6c\u6362\uff0c\u8fd4\u56de\u68c0\u6d4b\u6846\u4fe1\u606f ",(0,i.jsx)(n.code,{children:"[cls_id, score, x1, y1, x2, y2]"}),"\u3002"]}),"\n",(0,i.jsx)(n.h3,{id:"\u7ed8\u5236\u7ed3\u679c",children:"\u7ed8\u5236\u7ed3\u679c"}),"\n",(0,i.jsx)(n.p,{children:"\u5c06\u68c0\u6d4b\u6846\u6309\u6bd4\u4f8b\u7f29\u653e\u7ed8\u5236\u5230 OSD \u5c42\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"pl.osd_img.draw_rectangle(...)\npl.osd_img.draw_string_advanced(...)\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u6392\u9664\u4e86\u592a\u5c0f\u3001\u9760\u8fb9\u7684\u5e72\u6270\u6846\uff0c\u63d0\u5347\u68c0\u6d4b\u4f53\u9a8c\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u4e3b\u5faa\u73af",children:"\u4e3b\u5faa\u73af"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"while True:\n    img = pl.get_frame()\n    res = person_det.run(img)\n    person_det.draw_result(pl, res)\n    pl.show_image()\n    gc.collect()\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u6bcf\u5e27\u6267\u884c\u5b8c\u6574\u6d41\u7a0b\uff1a"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u83b7\u53d6\u5e27"}),"\n",(0,i.jsx)(n.li,{children:"\u56fe\u50cf\u9884\u5904\u7406 + \u63a8\u7406"}),"\n",(0,i.jsx)(n.li,{children:"\u540e\u5904\u7406\u89e3\u6790"}),"\n",(0,i.jsx)(n.li,{children:"\u7ed8\u5236\u7ed3\u679c"}),"\n",(0,i.jsx)(n.li,{children:"\u663e\u793a\u5230\u5c4f\u5e55"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming  # \u5bfc\u5165\u56fe\u50cf\u5904\u7406\u7ba1\u7ebf\u7c7b\u548c\u8ba1\u65f6\u5668\u7c7b\nfrom libs.AIBase import AIBase                    # \u5bfc\u5165AI\u63a8\u7406\u57fa\u7840\u7c7b\nfrom libs.AI2D import Ai2d                        # \u5bfc\u5165\u56fe\u50cf\u9884\u5904\u7406\u7c7b\nimport os\nimport ujson\nfrom media.media import *                         # \u5bfc\u5165\u5a92\u4f53\u5e93\uff08\u56fe\u50cf\u91c7\u96c6\u3001\u663e\u793a\u7b49\uff09\nfrom time import *\nimport nncase_runtime as nn                       # \u5bfc\u5165NNCase\u63a8\u7406\u5f15\u64ce\u5e93\nimport ulab.numpy as np                           # \u4f7f\u7528\u8f7b\u91cf\u5316numpy\u5e93ulab\uff0c\u9002\u914dMCU\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube                                     # \u5bfc\u5165aicube\u5e93\uff08\u5305\u542b\u540e\u5904\u7406\u51fd\u6570\uff09\n\n# \u81ea\u5b9a\u4e49\u4eba\u4f53\u68c0\u6d4b\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\nclass PersonDetectionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,labels,anchors,\n                 confidence_threshold=0.2,nms_threshold=0.5,nms_option=False,\n                 strides=[8,16,32],rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path = kmodel_path\n        self.model_input_size = model_input_size         # \u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\n        self.labels = labels                             # \u7c7b\u522b\u6807\u7b7e\u5217\u8868\n        self.anchors = anchors                           # \u951a\u6846\u914d\u7f6e\n        self.strides = strides                           # \u5404\u5c42\u8f93\u51fa\u7279\u5f81\u56fe\u5bf9\u5e94\u7684stride\n        self.confidence_threshold = confidence_threshold # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.nms_threshold = nms_threshold               # NMS\u6291\u5236\u9608\u503c\n        self.nms_option = nms_option                     # \u662f\u5426\u5f00\u542fNMS\u4f18\u5316\n        # \u8bbe\u7f6e\u56fe\u50cf\u91c7\u96c6\u5c3a\u5bf8\uff08\u5bf9\u9f50\u523016\uff09\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]\n        self.debug_mode = debug_mode\n\n        # \u521b\u5efaAi2D\u5b9e\u4f8b\u7528\u4e8e\u56fe\u50cf\u9884\u5904\u7406\uff08\u5982pad\u3001resize\u7b49\uff09\n        self.ai2d = Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ecpadding\u548cresize\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):\n            # \u4f7f\u7528\u9ed8\u8ba4\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u6216\u4f20\u5165\u7684\u81ea\u5b9a\u4e49\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u83b7\u53d6padding\u53c2\u6570\uff1atop, bottom, left, right\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [0, 0, 0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u7b97\u5b50\u56fe\n            self.ai2d.build([1, 3, ai2d_input_size[1], ai2d_input_size[0]],\n                            [1, 3, self.model_input_size[1], self.model_input_size[0]])\n\n    # \u6a21\u578b\u63a8\u7406\u7ed3\u679c\u7684\u540e\u5904\u7406\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            # \u4f7f\u7528aicube\u63d0\u4f9b\u7684anchor-based\u68c0\u6d4b\u540e\u5904\u7406\u51fd\u6570\n            dets = aicube.anchorbasedet_post_process(\n                results[0], results[1], results[2],\n                self.model_input_size, self.rgb888p_size,\n                self.strides, len(self.labels),\n                self.confidence_threshold, self.nms_threshold,\n                self.anchors, self.nms_option\n            )\n            return dets\n\n    # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\uff08\u7ed8\u5236\u77e9\u5f62\u6846\u4e0e\u7c7b\u522b\u6587\u5b57\uff09\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            if dets:\n                pl.osd_img.clear()\n                for det_box in dets:\n                    x1, y1, x2, y2 = det_box[2], det_box[3], det_box[4], det_box[5]\n                    w = float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0]\n                    h = float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1]\n                    x1 = int(x1 * self.display_size[0] // self.rgb888p_size[0])\n                    y1 = int(y1 * self.display_size[1] // self.rgb888p_size[1])\n                    x2 = int(x2 * self.display_size[0] // self.rgb888p_size[0])\n                    y2 = int(y2 * self.display_size[1] // self.rgb888p_size[1])\n\n                    # \u8fc7\u6ee4\u6389\u8fb9\u7f18\u5c0f\u76ee\u6807\u6216\u5f02\u5e38\u6846\n                    if (h < 0.1 * self.display_size[0]):\n                        continue\n                    if (w < 0.25 * self.display_size[0] and (x1 < 0.03 * self.display_size[0] or x2 > 0.97 * self.display_size[0])):\n                        continue\n                    if (w < 0.15 * self.display_size[0] and (x1 < 0.01 * self.display_size[0] or x2 > 0.99 * self.display_size[0])):\n                        continue\n\n                    # \u7ed8\u5236\u77e9\u5f62\u6846\u4e0e\u68c0\u6d4b\u6807\u7b7e\n                    pl.osd_img.draw_rectangle(x1, y1, int(w), int(h), color=(255, 0, 255, 0), thickness=2)\n                    pl.osd_img.draw_string_advanced(x1, y1 - 50, 32, " " + self.labels[det_box[0]] + " " + str(round(det_box[1], 2)), color=(255, 0, 255, 0))\n            else:\n                pl.osd_img.clear()\n\n    # \u8ba1\u7b97\u56fe\u50cf\u9884\u5904\u7406\u65f6\u7684padding\u586b\u5145\u503c\n    def get_padding_param(self):\n        dst_w, dst_h = self.model_input_size\n        input_width, input_high = self.rgb888p_size\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        ratio = min(ratio_w, ratio_h)\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw - 0.1))\n        return top, bottom, left, right\n\n# \u4e3b\u7a0b\u5e8f\u5165\u53e3\nif __name__ == "__main__":\n    display_mode = "lcd"  # \u663e\u793a\u6a21\u5f0f\uff0c\u53ef\u9009"hdmi"\u6216"lcd"\n    rgb888p_size = [1920, 1080]  # \u91c7\u96c6\u56fe\u50cf\u5206\u8fa8\u7387\n\n    # \u6839\u636e\u663e\u793a\u6a21\u5f0f\u8bbe\u7f6e\u663e\u793a\u5206\u8fa8\u7387\n    display_size = [1920, 1080] if display_mode == "hdmi" else [800, 480]\n\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path = "/sdcard/examples/kmodel/person_detect_yolov5n.kmodel"\n\n    # \u68c0\u6d4b\u53c2\u6570\u914d\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.6\n    labels = ["person"]\n    anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n\n    # \u521d\u59cb\u5316\u56fe\u50cf\u5904\u7406\u7ba1\u7ebf\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()\n\n    # \u521b\u5efa\u4eba\u4f53\u68c0\u6d4b\u5e94\u7528\u7c7b\u5b9e\u4f8b\u5e76\u521d\u59cb\u5316\n    person_det = PersonDetectionApp(\n        kmodel_path,\n        model_input_size=[640, 640],\n        labels=labels,\n        anchors=anchors,\n        confidence_threshold=confidence_threshold,\n        nms_threshold=nms_threshold,\n        nms_option=False,\n        strides=[8, 16, 32],\n        rgb888p_size=rgb888p_size,\n        display_size=display_size,\n        debug_mode=0\n    )\n    person_det.config_preprocess()\n\n    try:\n        while True:\n            os.exitpoint()  # \u68c0\u67e5\u9000\u51fa\u70b9\n            with ScopedTiming("total", 1):\n                img = pl.get_frame()              # \u91c7\u96c6\u5f53\u524d\u56fe\u50cf\u5e27\n                res = person_det.run(img)         # \u6267\u884c\u6a21\u578b\u63a8\u7406\n                person_det.draw_result(pl, res)   # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                   # \u663e\u793a\u56fe\u50cf\n                gc.collect()                      # \u56de\u6536\u5185\u5b58\n    except Exception as e:\n        sys.print_exception(e)                   # \u5f02\u5e38\u6253\u5370\n    finally:\n        person_det.deinit()                      # \u6a21\u578b\u53cd\u521d\u59cb\u5316\n        pl.destroy()                             # \u9500\u6bc1\u56fe\u50cf\u7ba1\u7ebf\n'})}),"\n",(0,i.jsx)(n.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"person_274",src:s(11177).Z+"",width:"640",height:"480"})}),"\n",(0,i.jsx)(n.p,{children:"\u200b\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u4e0a\u770b\u5230\u4eba\u5f62\u68c0\u6d4b\u7684\u7ed3\u679c\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250423165257731",src:s(43659).Z+"",width:"812",height:"500"})})]})}function c(e={}){const{wrapper:n}={...(0,d.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},43659:(e,n,s)=>{s.d(n,{Z:()=>i});const i=s.p+"assets/images/image-20250423165257731-a69e95c7d382fc01558b37043e3ebc79.png"},11177:(e,n,s)=>{s.d(n,{Z:()=>i});const i=s.p+"assets/images/person_274-45bdaf0c62bd5a2ea4e075a900c31b5c.png"},11151:(e,n,s)=>{s.d(n,{Z:()=>r,a:()=>t});var i=s(67294);const d={},l=i.createContext(d);function t(e){const n=i.useContext(l);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:t(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);