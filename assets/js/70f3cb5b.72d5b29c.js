"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9791],{60933:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>_,contentTitle:()=>d,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var s=i(85893),t=i(11151);const r={sidebar_position:6},d="\u4eba\u8138\u59ff\u6001\u4f30\u8ba1",a={id:"CanaanK230/part14/part2/facePose",title:"\u4eba\u8138\u59ff\u6001\u4f30\u8ba1",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part2/06-facePose.md",sourceDirName:"CanaanK230/part14/part2",slug:"/CanaanK230/part14/part2/facePose",permalink:"/CanaanK230/part14/part2/facePose",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part2/06-facePose.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"canaanK230Sidebar",previous:{title:"\u4eba\u8138\u89e3\u6790\uff08\u5206\u5272\uff09",permalink:"/CanaanK230/part14/part2/faceParse"},next:{title:"\u4eba\u8138\u8bc6\u522b",permalink:"/CanaanK230/part14/part2/faceRegistration"}},_={},l=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u6838\u5fc3\u4ee3\u7801",id:"2\u6838\u5fc3\u4ee3\u7801",level:2},{value:"\u9884\u5904\u7406",id:"\u9884\u5904\u7406",level:3},{value:"\u4eff\u5c04\u53d8\u6362",id:"\u4eff\u5c04\u53d8\u6362",level:3},{value:"\u540e\u5904\u7406",id:"\u540e\u5904\u7406",level:3},{value:"\u8f6c\u6b27\u62c9\u89d2",id:"\u8f6c\u6b27\u62c9\u89d2",level:3},{value:"\u63d0\u53d6\u65cb\u8f6c\u77e9\u9635",id:"\u63d0\u53d6\u65cb\u8f6c\u77e9\u9635",level:3},{value:"\u6838\u5fc3\u903b\u8f91",id:"\u6838\u5fc3\u903b\u8f91",level:3},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function o(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"\u4eba\u8138\u59ff\u6001\u4f30\u8ba1",children:"\u4eba\u8138\u59ff\u6001\u4f30\u8ba1"}),"\n",(0,s.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,s.jsx)(n.p,{children:"\u901a\u8fc7\u6444\u50cf\u5934\u6355\u83b7\u56fe\u50cf\uff0c\u5b9a\u4f4d\u4eba\u8138\uff0c\u5e76\u8ba1\u7b97\u5176\u59ff\u6001\u4fe1\u606f\uff0c\u5305\u62ec\u4fef\u4ef0\u89d2\uff08Pitch\uff09\u3001\u504f\u822a\u89d2\uff08Yaw\uff09\u548c\u6eda\u8f6c\u89d2\uff08Roll\uff09\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"2\u6838\u5fc3\u4ee3\u7801",children:"2.\u6838\u5fc3\u4ee3\u7801"}),"\n",(0,s.jsx)(n.h3,{id:"\u9884\u5904\u7406",children:"\u9884\u5904\u7406"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\u5e76\u8bbe\u7f6eaffine\u9884\u5904\u7406\n            matrix_dst = self.get_affine_matrix(det)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,matrix_dst)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u9009\u62e9\u9884\u5904\u7406\u8f93\u5165\u5c3a\u5bf8"}),"\uff1a"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u9ed8\u8ba4\u4e3a\u539f\u56fe\u50cf\u5c3a\u5bf8 ",(0,s.jsx)(n.code,{children:"rgb888p_size"}),"\uff0c\u4e5f\u53ef\u624b\u52a8\u6307\u5b9a ",(0,s.jsx)(n.code,{children:"input_image_size"}),"\u3002"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u4eff\u5c04\u77e9\u9635\u751f\u6210"}),"\uff1a"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u8c03\u7528 ",(0,s.jsx)(n.code,{children:"get_affine_matrix(det)"}),"\uff0c\u6839\u636e\u4eba\u8138\u6846 bbox \u751f\u6210 affine warp \u6240\u9700\u7684 2x3 \u4eff\u5c04\u77e9\u9635\u3002"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u8bbe\u7f6e affine \u53c2\u6570"}),"\uff1a"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u4f7f\u7528 ",(0,s.jsx)(n.code,{children:"cv2_bilinear"})," \u63d2\u503c\uff0c\u80cc\u666f\u586b\u5145\u4e3a\u7070\u8272\uff08127\uff09\u3002"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u6784\u5efa\u6d41\u7a0b\u56fe"}),"\uff1a"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"build()"})," \u63a5\u6536\u8f93\u5165 tensor \u548c\u8f93\u51fa tensor \u7684 shape\uff0c\u7528\u4e8e\u9884\u5904\u7406\u6a21\u5757\u751f\u6210\u56fe\u50cf\u6d41\u3002"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\u4eff\u5c04\u53d8\u6362",children:"\u4eff\u5c04\u53d8\u6362"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'    def get_affine_matrix(self,bbox):\n        # \u83b7\u53d6\u4eff\u5c04\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50\n            factor = 2.7\n            # \u4ece\u8fb9\u754c\u6846\u63d0\u53d6\u5750\u6807\u548c\u5c3a\u5bf8\n            x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n            # \u6a21\u578b\u8f93\u5165\u5927\u5c0f\n            edge_size = self.model_input_size[1]\n            # \u5e73\u79fb\u8ddd\u79bb\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e2d\u5fc3\u5bf9\u51c6\u539f\u70b9\n            trans_distance = edge_size / 2.0\n            # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u7684\u5750\u6807\n            center_x = x1 + w / 2.0\n            center_y = y1 + h / 2.0\n            # \u8ba1\u7b97\u6700\u5927\u8fb9\u957f\n            maximum_edge = factor * (h if h > w else w)\n            # \u8ba1\u7b97\u7f29\u653e\u6bd4\u4f8b\n            scale = edge_size * 2.0 / maximum_edge\n            # \u8ba1\u7b97\u5e73\u79fb\u53c2\u6570\n            cx = trans_distance - scale * center_x\n            cy = trans_distance - scale * center_y\n            # \u521b\u5efa\u4eff\u5c04\u77e9\u9635\n            affine_matrix = [scale, 0, cx, 0, scale, cy]\n            return affine_matrix\n'})}),"\n",(0,s.jsxs)(n.p,{children:["\u4f7f\u7528\u4e00\u4e2a\u7f29\u653e\u56e0\u5b50 ",(0,s.jsx)(n.code,{children:"factor = 2.7"}),"\uff0c\u9002\u5f53\u6269\u5927\u4eba\u8138\u533a\u57df\u4ee5\u9002\u914d\u6a21\u578b\u671f\u671b\u8f93\u5165\u3002"]}),"\n",(0,s.jsx)(n.p,{children:"\u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u5750\u6807"}),"\n",(0,s.jsxs)(n.p,{children:["\u7f29\u653e\u6bd4\u4f8b ",(0,s.jsx)(n.code,{children:"scale = (edge_size * 2.0) / maximum_edge"}),"\uff1a\u786e\u4fdd\u4eba\u8138\u5b8c\u6574\u663e\u793a\u5728\u6a21\u578b\u8f93\u5165\u533a\u57df"]}),"\n",(0,s.jsx)(n.p,{children:"\u504f\u79fb\u91cf cx, cy \u8ba1\u7b97\u7528\u4e8e\u5e73\u79fb"}),"\n",(0,s.jsxs)(n.p,{children:["\u6784\u5efa\u4eff\u5c04\u77e9\u9635 ",(0,s.jsx)(n.code,{children:"affine_matrix = [scale, 0, cx, 0, scale, cy]"}),"\uff0c\u8868\u793a\u4eff\u5c04\u53d8\u6362\u7684 2\xd73 \u77e9\u9635"]}),"\n",(0,s.jsx)(n.h3,{id:"\u540e\u5904\u7406",children:"\u540e\u5904\u7406"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            R,eular = self.get_euler(results[0][0])\n            return R,eular\n'})}),"\n",(0,s.jsx)(n.p,{children:"\u4ece\u6a21\u578b\u8f93\u51fa\uff08\u7ed3\u679c tensor\uff09\u4e2d\u63d0\u53d6\u65cb\u8f6c\u77e9\u9635\u5e76\u8f6c\u6362\u4e3a\u6b27\u62c9\u89d2"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"results[0][0]"}),"\uff1a\u901a\u5e38\u8868\u793a\u6a21\u578b\u8f93\u51fa\u7684\u7b2c\u4e00\u4e2a batch\u3001\u7b2c\u4e00\u4e2a\u6837\u672c\uff0c\u5373 3\xd73 \u7684 rotation matrix\uff08\u65cb\u8f6c\u77e9\u9635\uff09"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"get_euler()"}),"\uff1a\u4ece rotation matrix \u4e2d\u63d0\u53d6\u6b27\u62c9\u89d2\uff08Pitch, Yaw, Roll\uff09"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\u8f6c\u6b27\u62c9\u89d2",children:"\u8f6c\u6b27\u62c9\u89d2"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"    def rotation_matrix_to_euler_angles(self,R):\n        # \u5c06\u65cb\u8f6c\u77e9\u9635\uff083x3 \u77e9\u9635\uff09\u8f6c\u6362\u4e3a\u6b27\u62c9\u89d2\uff08pitch\u3001yaw\u3001roll\uff09\n        # \u8ba1\u7b97 sin(yaw)\n        sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)\n        if sy < 1e-6:\n            # \u82e5 sin(yaw) \u8fc7\u5c0f\uff0c\u8bf4\u660e pitch \u63a5\u8fd1 \xb190 \u5ea6\n            pitch = np.arctan2(-R[1, 2], R[1, 1]) * 180 / np.pi\n            yaw = np.arctan2(-R[2, 0], sy) * 180 / np.pi\n            roll = 0\n        else:\n            # \u8ba1\u7b97 pitch\u3001yaw\u3001roll \u7684\u89d2\u5ea6\n            pitch = np.arctan2(R[2, 1], R[2, 2]) * 180 / np.pi\n            yaw = np.arctan2(-R[2, 0], sy) * 180 / np.pi\n            roll = np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi\n        return [pitch,yaw,roll]\n"})}),"\n",(0,s.jsxs)(n.p,{children:["\u5c06\u65cb\u8f6c\u77e9\u9635 ",(0,s.jsx)(n.code,{children:"R"}),"\uff083\xd73\uff09\u8f6c\u5316\u4e3a\u6b27\u62c9\u89d2\uff08pitch\u3001yaw\u3001roll\uff09\u3002"]}),"\n",(0,s.jsxs)(n.p,{children:["\u57fa\u4e8e\u7ecf\u5178\u7684\u4e09\u7ef4\u65cb\u8f6c\u77e9\u9635\u8f6c\u6b27\u62c9\u89d2\u7684\u516c\u5f0f\uff0c\u5982\u679c\u7ed5 ",(0,s.jsx)(n.code,{children:"Y"})," \u8f74\u7684\u65cb\u8f6c sin \u503c ",(0,s.jsx)(n.code,{children:"sy"})," \u8fc7\u5c0f\uff0c\u5219\u8ba4\u4e3a\u59ff\u6001\u5904\u4e8e\u5947\u5f02\u72b6\u6001\uff08Gimbal lock\uff09\uff0c\u9700\u7279\u6b8a\u5904\u7406\u3002"]}),"\n",(0,s.jsx)(n.h3,{id:"\u63d0\u53d6\u65cb\u8f6c\u77e9\u9635",children:"\u63d0\u53d6\u65cb\u8f6c\u77e9\u9635"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"    def get_euler(self,data):\n        # \u83b7\u53d6\u65cb\u8f6c\u77e9\u9635\u548c\u6b27\u62c9\u89d2\n        R = data[:3, :3].copy()\n        eular = self.rotation_matrix_to_euler_angles(R)\n        return R,eular\n"})}),"\n",(0,s.jsx)(n.p,{children:"\u63d0\u53d6\u65cb\u8f6c\u77e9\u9635\u7684\u524d\u4e09\u884c\u4e09\u5217\uff083\xd73\uff09\uff0c\u8c03\u7528\u4e0a\u9762\u7684\u51fd\u6570\u83b7\u53d6\u6b27\u62c9\u89d2"}),"\n",(0,s.jsx)(n.h3,{id:"\u6838\u5fc3\u903b\u8f91",children:"\u6838\u5fc3\u903b\u8f91"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"         +---------------------+\n         | \u6355\u83b7\u56fe\u50cf (camera)  |\n         +---------+-----------+\n                   |\n                   v\n         +---------------------+\n         | \u4eba\u8138\u68c0\u6d4b -> BBox    |\n         +---------------------+\n                   |\n                   v\n         +---------------------------+\n         | get_affine_matrix(bbox)  |\n         | -> \u6784\u5efa\u4eff\u5c04\u77e9\u9635          |\n         +---------------------------+\n                   |\n                   v\n         +---------------------------+\n         | AI2D.affine + build       |\n         | \u56fe\u50cf\u9884\u5904\u7406                |\n         +---------------------------+\n                   |\n                   v\n         +--------------------------+\n         | \u6a21\u578b\u63a8\u7406 (kmodel)       |\n         +--------------------------+\n                   |\n                   v\n         +--------------------------+\n         | \u63d0\u53d6\u65cb\u8f6c\u77e9\u9635 -> \u6b27\u62c9\u89d2  |\n         +--------------------------+\n"})}),"\n",(0,s.jsx)(n.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0\uff1a DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u59ff\u6001\u4efb\u52a1\u7c7b\nclass FacePoseApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\u5e76\u8bbe\u7f6eaffine\u9884\u5904\u7406\n            matrix_dst = self.get_affine_matrix(det)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,matrix_dst)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8ba1\u7b97\u6b27\u62c9\u89d2\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            R,eular = self.get_euler(results[0][0])\n            return R,eular\n\n    def get_affine_matrix(self,bbox):\n        # \u83b7\u53d6\u4eff\u5c04\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50\n            factor = 2.7\n            # \u4ece\u8fb9\u754c\u6846\u63d0\u53d6\u5750\u6807\u548c\u5c3a\u5bf8\n            x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n            # \u6a21\u578b\u8f93\u5165\u5927\u5c0f\n            edge_size = self.model_input_size[1]\n            # \u5e73\u79fb\u8ddd\u79bb\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e2d\u5fc3\u5bf9\u51c6\u539f\u70b9\n            trans_distance = edge_size / 2.0\n            # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u7684\u5750\u6807\n            center_x = x1 + w / 2.0\n            center_y = y1 + h / 2.0\n            # \u8ba1\u7b97\u6700\u5927\u8fb9\u957f\n            maximum_edge = factor * (h if h > w else w)\n            # \u8ba1\u7b97\u7f29\u653e\u6bd4\u4f8b\n            scale = edge_size * 2.0 / maximum_edge\n            # \u8ba1\u7b97\u5e73\u79fb\u53c2\u6570\n            cx = trans_distance - scale * center_x\n            cy = trans_distance - scale * center_y\n            # \u521b\u5efa\u4eff\u5c04\u77e9\u9635\n            affine_matrix = [scale, 0, cx, 0, scale, cy]\n            return affine_matrix\n\n    def rotation_matrix_to_euler_angles(self,R):\n        # \u5c06\u65cb\u8f6c\u77e9\u9635\uff083x3 \u77e9\u9635\uff09\u8f6c\u6362\u4e3a\u6b27\u62c9\u89d2\uff08pitch\u3001yaw\u3001roll\uff09\n        # \u8ba1\u7b97 sin(yaw)\n        sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)\n        if sy < 1e-6:\n            # \u82e5 sin(yaw) \u8fc7\u5c0f\uff0c\u8bf4\u660e pitch \u63a5\u8fd1 \xb190 \u5ea6\n            pitch = np.arctan2(-R[1, 2], R[1, 1]) * 180 / np.pi\n            yaw = np.arctan2(-R[2, 0], sy) * 180 / np.pi\n            roll = 0\n        else:\n            # \u8ba1\u7b97 pitch\u3001yaw\u3001roll \u7684\u89d2\u5ea6\n            pitch = np.arctan2(R[2, 1], R[2, 2]) * 180 / np.pi\n            yaw = np.arctan2(-R[2, 0], sy) * 180 / np.pi\n            roll = np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi\n        return [pitch,yaw,roll]\n\n    def get_euler(self,data):\n        # \u83b7\u53d6\u65cb\u8f6c\u77e9\u9635\u548c\u6b27\u62c9\u89d2\n        R = data[:3, :3].copy()\n        eular = self.rotation_matrix_to_euler_angles(R)\n        return R,eular\n\n# \u4eba\u8138\u59ff\u6001\u4efb\u52a1\u7c7b\nclass FacePose:\n    def __init__(self,face_det_kmodel,face_pose_kmodel,det_input_size,pose_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8def\u5f84\n        self.face_pose_kmodel=face_pose_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.pose_input_size=pose_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.face_pose=FacePoseApp(self.face_pose_kmodel,model_input_size=self.pose_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        pose_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u4eba\u8138\u505a\u4eba\u8138\u59ff\u6001\u4f30\u8ba1\n            self.face_pose.config_preprocess(det_box)\n            R,eular=self.face_pose.run(input_np)\n            pose_res.append((R,eular))\n        return det_boxes,pose_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u59ff\u6001\u89d2\u6548\u679c\n    def draw_result(self,pl,dets,pose_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=draw_img_np)\n            line_color = np.array([255, 0, 0 ,255],dtype=np.uint8)    #bgra\n            for i,det in enumerate(dets):\n                # \uff081\uff09\u83b7\u53d6\u4eba\u8138\u59ff\u6001\u77e9\u9635\u548c\u6b27\u62c9\u89d2\n                projections,center_point = self.build_projection_matrix(det)\n                R,euler = pose_res[i]\n                # \uff082\uff09\u904d\u5386\u4eba\u8138\u6295\u5f71\u77e9\u9635\u7684\u5173\u952e\u70b9\uff0c\u8fdb\u884c\u6295\u5f71\uff0c\u5e76\u5c06\u7ed3\u679c\u753b\u5728\u56fe\u50cf\u4e0a\n                first_points = []\n                second_points = []\n                for pp in range(8):\n                    sum_x, sum_y = 0.0, 0.0\n                    for cc in range(3):\n                        sum_x += projections[pp][cc] * R[cc][0]\n                        sum_y += projections[pp][cc] * (-R[cc][1])\n                    center_x,center_y = center_point[0],center_point[1]\n                    x = (sum_x + center_x) / self.rgb888p_size[0] * self.display_size[0]\n                    y = (sum_y + center_y) / self.rgb888p_size[1] * self.display_size[1]\n                    x = max(0, min(x, self.display_size[0]))\n                    y = max(0, min(y, self.display_size[1]))\n                    if pp < 4:\n                        first_points.append((x, y))\n                    else:\n                        second_points.append((x, y))\n                first_points = np.array(first_points,dtype=np.float)\n                aidemo.polylines(draw_img_np,first_points,True,line_color,2,8,0)\n                second_points = np.array(second_points,dtype=np.float)\n                aidemo.polylines(draw_img_np,second_points,True,line_color,2,8,0)\n                for ll in range(4):\n                    x0, y0 = int(first_points[ll][0]),int(first_points[ll][1])\n                    x1, y1 = int(second_points[ll][0]),int(second_points[ll][1])\n                    draw_img.draw_line(x0, y0, x1, y1, color = (255, 0, 0 ,255), thickness = 2)\n            pl.osd_img.copy_from(draw_img)\n\n    def build_projection_matrix(self,det):\n        x1, y1, w, h = map(lambda x: int(round(x, 0)), det[:4])\n        # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u5750\u6807\n        center_x = x1 + w / 2.0\n        center_y = y1 + h / 2.0\n        # \u5b9a\u4e49\u540e\u90e8\uff08rear\uff09\u548c\u524d\u90e8\uff08front\uff09\u7684\u5c3a\u5bf8\u548c\u6df1\u5ea6\n        rear_width = 0.5 * w\n        rear_height = 0.5 * h\n        rear_depth = 0\n        factor = np.sqrt(2.0)\n        front_width = factor * rear_width\n        front_height = factor * rear_height\n        front_depth = factor * rear_width  # \u4f7f\u7528\u5bbd\u5ea6\u6765\u8ba1\u7b97\u6df1\u5ea6\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528\u9ad8\u5ea6\uff0c\u53d6\u51b3\u4e8e\u9700\u6c42\n        # \u5b9a\u4e49\u7acb\u65b9\u4f53\u7684\u9876\u70b9\u5750\u6807\n        temp = [\n            [-rear_width, -rear_height, rear_depth],\n            [-rear_width, rear_height, rear_depth],\n            [rear_width, rear_height, rear_depth],\n            [rear_width, -rear_height, rear_depth],\n            [-front_width, -front_height, front_depth],\n            [-front_width, front_height, front_depth],\n            [front_width, front_height, front_depth],\n            [front_width, -front_height, front_depth]\n        ]\n        projections = np.array(temp)\n        # \u8fd4\u56de\u6295\u5f71\u77e9\u9635\u548c\u4e2d\u5fc3\u5750\u6807\n        return projections, (center_x, center_y)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="lcd"\n    # k230\u4fdd\u6301\u4e0d\u53d8\uff0ck230d\u53ef\u8c03\u6574\u4e3a[640,360]\n    rgb888p_size = [1920, 1080]\n\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/examples/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8def\u5f84\n    face_pose_kmodel_path="/sdcard/examples/kmodel/face_pose.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    face_det_input_size=[320,320]\n    face_pose_input_size=[120,120]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    fp=FacePose(face_det_kmodel_path,face_pose_kmodel_path,det_input_size=face_det_input_size,pose_input_size=face_pose_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,pose_res=fp.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                fp.draw_result(pl,det_boxes,pose_res)   # \u7ed8\u5236\u63a8\u7406\u6548\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fp.face_det.deinit()\n        fp.face_pose.deinit()\n        pl.destroy()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"person_122",src:i(74935).Z+"",width:"480",height:"640"})}),"\n",(0,s.jsx)(n.p,{children:"\u200b\t\u70b9\u51fb\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u4e0a\u770b\u5230\u4eba\u8138\u59ff\u6001\u4f30\u8ba1\u7684\u7ed3\u679c\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image-20250423173014833",src:i(81006).Z+"",width:"1318",height:"913"})})]})}function p(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},81006:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/image-20250423173014833-18ddeb5d21f8b031701ac6ea515ed418.png"},74935:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/person_122-a3ff7e57fa864e733dfcd8dc2701c639.png"},11151:(e,n,i)=>{i.d(n,{Z:()=>a,a:()=>d});var s=i(67294);const t={},r=s.createContext(t);function d(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);