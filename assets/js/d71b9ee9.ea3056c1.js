"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3934],{68340:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>p,contentTitle:()=>r,default:()=>l,frontMatter:()=>_,metadata:()=>d,toc:()=>o});var s=i(85893),t=i(11151);const _={sidebar_position:2},r="OCR\u6587\u5b57\u8bc6\u522b",d={id:"CanaanK230/part14/part5/ocrRec",title:"OCR\u6587\u5b57\u8bc6\u522b",description:"1.\u5b9e\u9a8c\u76ee\u7684",source:"@site/docs/CanaanK230/part14/part5/02-ocrRec.md",sourceDirName:"CanaanK230/part14/part5",slug:"/CanaanK230/part14/part5/ocrRec",permalink:"/CanaanK230/part14/part5/ocrRec",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/eLinuxAI-TrainingDocs/tree/main/docs/CanaanK230/part14/part5/02-ocrRec.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"canaanK230Sidebar",previous:{title:"OCR\u6587\u5b57\u68c0\u6d4b",permalink:"/CanaanK230/part14/part5/ocrDet"},next:{title:"YOLOV8",permalink:"/category/yolov8"}},p={},o=[{value:"1.\u5b9e\u9a8c\u76ee\u7684",id:"1\u5b9e\u9a8c\u76ee\u7684",level:2},{value:"2.\u6838\u5fc3\u6d41\u7a0b",id:"2\u6838\u5fc3\u6d41\u7a0b",level:2},{value:"3.\u793a\u4f8b\u4ee3\u7801",id:"3\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.\u5b9e\u9a8c\u7ed3\u679c",id:"4\u5b9e\u9a8c\u7ed3\u679c",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"ocr\u6587\u5b57\u8bc6\u522b",children:"OCR\u6587\u5b57\u8bc6\u522b"}),"\n",(0,s.jsx)(n.h2,{id:"1\u5b9e\u9a8c\u76ee\u7684",children:"1.\u5b9e\u9a8c\u76ee\u7684"}),"\n",(0,s.jsx)(n.p,{children:"\u5b9e\u73b0\u6444\u50cf\u5934\u753b\u9762\u7684\u5b57\u7b26\u8bc6\u522b\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"2\u6838\u5fc3\u6d41\u7a0b",children:"2.\u6838\u5fc3\u6d41\u7a0b"}),"\n",(0,s.jsx)(n.p,{children:"\u914d\u7f6e\u663e\u793a\u6a21\u5f0f\u3001\u5206\u8fa8\u7387\u4ee5\u53caOCR\u6a21\u578b\u8def\u5f84\u3002"}),"\n",(0,s.jsxs)(n.p,{children:["\u521b\u5efa",(0,s.jsx)(n.code,{children:"PipeLine"}),"\u5b9e\u4f8b\uff0c\u7528\u4e8e\u83b7\u53d6\u89c6\u9891\u5e27\u3002"]}),"\n",(0,s.jsxs)(n.p,{children:["\u521d\u59cb\u5316",(0,s.jsx)(n.code,{children:"OCRDetRec"}),"\uff0c\u5f00\u59cb\u56fe\u50cf\u7684OCR\u68c0\u6d4b\u548c\u8bc6\u522b\u6d41\u7a0b\u3002"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OCR\u68c0\u6d4b"}),"\uff1a\u9996\u5148\u4f7f\u7528",(0,s.jsx)(n.code,{children:"ocr_det"}),"\u8fdb\u884c\u6587\u672c\u533a\u57df\u7684\u68c0\u6d4b\u3002"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OCR\u8bc6\u522b"}),"\uff1a\u7136\u540e\u5bf9\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u6587\u672c\u6846\u8fdb\u884c\u5b57\u7b26\u8bc6\u522b\uff0c\u5f97\u5230OCR\u5b57\u7b26\u4e32\u3002"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"3\u793a\u4f8b\u4ee3\u7801",children:"3.\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\'\'\'\n\u672c\u7a0b\u5e8f\u9075\u5faaGPL V3\u534f\u8bae, \u8bf7\u9075\u5faa\u534f\u8bae\n\u5b9e\u9a8c\u5e73\u53f0: DshanPI CanMV\n\u5f00\u53d1\u677f\u6587\u6863\u7ad9\u70b9\t: https://eai.100ask.net/\n\u767e\u95ee\u7f51\u5b66\u4e60\u5e73\u53f0   : https://www.100ask.net\n\u767e\u95ee\u7f51\u5b98\u65b9B\u7ad9    : https://space.bilibili.com/275908810\n\u767e\u95ee\u7f51\u5b98\u65b9\u6dd8\u5b9d   : https://100ask.taobao.com\n\'\'\'\nfrom libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49OCR\u68c0\u6d4b\u7c7b\nclass OCRDetectionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,mask_threshold=0.5,box_threshold=0.2,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u5206\u7c7b\u9608\u503c\n        self.mask_threshold=mask_threshold\n        self.box_threshold=box_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # chw2hwc\n            hwc_array=self.chw2hwc(self.cur_img)\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5c01\u88c5\u7684\u63a5\u53e3ocr_post_process\u505a\u540e\u5904\u7406,\u8fd4\u56de\u7684det_boxes\u7ed3\u6784\u4e3a[[crop_array_nhwc,[p1_x,p1_y,p2_x,p2_y,p3_x,p3_y,p4_x,p4_y]],...]\n            det_boxes = aicube.ocr_post_process(results[0][:,:,:,0].reshape(-1), hwc_array.reshape(-1),self.model_input_size,self.rgb888p_size, self.mask_threshold, self.box_threshold)\n            return det_boxes\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        # \u53f3padding\u6216\u4e0bpadding\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return  top, bottom, left, right\n\n    # chw2hwc\n    def chw2hwc(self,features):\n        ori_shape = (features.shape[0], features.shape[1], features.shape[2])\n        c_hw_ = features.reshape((ori_shape[0], ori_shape[1] * ori_shape[2]))\n        hw_c_ = c_hw_.transpose()\n        new_array = hw_c_.copy()\n        hwc_array = new_array.reshape((ori_shape[1], ori_shape[2], ori_shape[0]))\n        del c_hw_\n        del hw_c_\n        del new_array\n        return hwc_array\n\n# \u81ea\u5b9a\u4e49OCR\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass OCRRecognitionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,dict_path,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        self.dict_path=dict_path\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.dict_word=None\n        # \u8bfb\u53d6OCR\u7684\u5b57\u5178\n        self.read_dict()\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.RGB_packed,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None,input_np=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param(ai2d_input_size,self.model_input_size)\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u5982\u679c\u4f20\u5165input_np\uff0c\u8f93\u5165shape\u4e3ainput_np\u7684shape,\u5982\u679c\u4e0d\u4f20\u5165\uff0c\u8f93\u5165shape\u4e3a[1,3,ai2d_input_size[1],ai2d_input_size[0]]\n            self.ai2d.build([input_np.shape[0],input_np.shape[1],input_np.shape[2],input_np.shape[3]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            preds = np.argmax(results[0], axis=2).reshape((-1))\n            output_txt = ""\n            for i in range(len(preds)):\n                # \u5f53\u524d\u8bc6\u522b\u5b57\u7b26\u4e0d\u662f\u5b57\u5178\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\u5e76\u4e14\u548c\u524d\u4e00\u4e2a\u5b57\u7b26\u4e0d\u91cd\u590d\uff08\u53bb\u91cd\uff09\uff0c\u52a0\u5165\u8bc6\u522b\u7ed3\u679c\u5b57\u7b26\u4e32\n                if preds[i] != (len(self.dict_word) - 1) and (not (i > 0 and preds[i - 1] == preds[i])):\n                    output_txt = output_txt + self.dict_word[preds[i]]\n            return output_txt\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self,src_size,dst_size):\n        # \u53f3padding\u6216\u4e0bpadding\n        dst_w = dst_size[0]\n        dst_h = dst_size[1]\n        input_width = src_size[0]\n        input_high = src_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return  top, bottom, left, right\n\n    def read_dict(self):\n        if self.dict_path!="":\n            with open(dict_path, \'r\') as file:\n                line_one = file.read(100000)\n                line_list = line_one.split("\\r\\n")\n            self.dict_word = {num: char.replace("\\r", "").replace("\\n", "") for num, char in enumerate(line_list)}\n\n\nclass OCRDetRec:\n    def __init__(self,ocr_det_kmodel,ocr_rec_kmodel,det_input_size,rec_input_size,dict_path,mask_threshold=0.25,box_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # OCR\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.ocr_det_kmodel=ocr_det_kmodel\n        # OCR\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n        self.ocr_rec_kmodel=ocr_rec_kmodel\n        # OCR\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # OCR\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.rec_input_size=rec_input_size\n        # \u5b57\u5178\u8def\u5f84\n        self.dict_path=dict_path\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.mask_threshold=mask_threshold\n        # nms\u9608\u503c\n        self.box_threshold=box_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ocr_det=OCRDetectionApp(self.ocr_det_kmodel,model_input_size=self.det_input_size,mask_threshold=self.mask_threshold,box_threshold=self.box_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.ocr_rec=OCRRecognitionApp(self.ocr_rec_kmodel,model_input_size=self.rec_input_size,dict_path=self.dict_path,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.ocr_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5148\u8fdb\u884cOCR\u68c0\u6d4b\n        det_res=self.ocr_det.run(input_np)\n        boxes=[]\n        ocr_res=[]\n        for det in det_res:\n            # \u5bf9\u5f97\u5230\u7684\u6bcf\u4e2a\u68c0\u6d4b\u6846\u6267\u884cOCR\u8bc6\u522b\n            self.ocr_rec.config_preprocess(input_image_size=[det[0].shape[2],det[0].shape[1]],input_np=det[0])\n            ocr_str=self.ocr_rec.run(det[0])\n            ocr_res.append(ocr_str)\n            boxes.append(det[1])\n            gc.collect()\n        return boxes,ocr_res\n\n    # \u7ed8\u5236OCR\u68c0\u6d4b\u8bc6\u522b\u6548\u679c\n    def draw_result(self,pl,det_res,rec_res):\n        pl.osd_img.clear()\n        if det_res:\n            # \u5faa\u73af\u7ed8\u5236\u6240\u6709\u68c0\u6d4b\u5230\u7684\u6846\n            for j in range(len(det_res)):\n                # \u5c06\u539f\u56fe\u7684\u5750\u6807\u70b9\u8f6c\u6362\u6210\u663e\u793a\u7684\u5750\u6807\u70b9\uff0c\u5faa\u73af\u7ed8\u5236\u56db\u6761\u76f4\u7ebf\uff0c\u5f97\u5230\u4e00\u4e2a\u77e9\u5f62\u6846\n                for i in range(4):\n                    x1 = det_res[j][(i * 2)] / self.rgb888p_size[0] * self.display_size[0]\n                    y1 = det_res[j][(i * 2 + 1)] / self.rgb888p_size[1] * self.display_size[1]\n                    x2 = det_res[j][((i + 1) * 2) % 8] / self.rgb888p_size[0] * self.display_size[0]\n                    y2 = det_res[j][((i + 1) * 2 + 1) % 8] / self.rgb888p_size[1] * self.display_size[1]\n                    pl.osd_img.draw_line((int(x1), int(y1), int(x2), int(y2)), color=(255, 0, 0, 255),thickness=5)\n                pl.osd_img.draw_string_advanced(int(x1),int(y1),32,rec_res[j],color=(0,0,255))\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\uff0ck230d\u53d7\u9650\u5185\u5b58\u4e0d\u652f\u6301\n    display_mode="lcd"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # OCR\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    ocr_det_kmodel_path="/sdcard/examples/kmodel/ocr_det_int16.kmodel"\n    # OCR\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    ocr_rec_kmodel_path="/sdcard/examples/kmodel/ocr_rec_int16.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    dict_path="/sdcard/examples/utils/dict.txt"\n    rgb888p_size=[640,360]\n    ocr_det_input_size=[640,640]\n    ocr_rec_input_size=[512,32]\n    mask_threshold=0.25\n    box_threshold=0.3\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    ocr=OCRDetRec(ocr_det_kmodel_path,ocr_rec_kmodel_path,det_input_size=ocr_det_input_size,rec_input_size=ocr_rec_input_size,dict_path=dict_path,mask_threshold=mask_threshold,box_threshold=box_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                  # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_res,rec_res=ocr.run(img)        # \u63a8\u7406\u5f53\u524d\u5e27\n                ocr.draw_result(pl,det_res,rec_res) # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                     # \u5c55\u793a\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        ocr.ocr_det.deinit()\n        ocr.ocr_rec.deinit()\n        pl.destroy()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"4\u5b9e\u9a8c\u7ed3\u679c",children:"4.\u5b9e\u9a8c\u7ed3\u679c"}),"\n",(0,s.jsxs)(n.p,{children:["\u200b\t",(0,s.jsx)(n.img,{alt:"image-20250423182608052",src:i(86006).Z+"",width:"708",height:"398"})]}),"\n",(0,s.jsx)(n.p,{children:"\u200b\t\u8fd0\u884c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5728\u663e\u793a\u5c4f\u770b\u5230\u8bc6\u522b\u7ed3\u679c\uff1a"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image-20250423183009683",src:i(14538).Z+"",width:"1379",height:"925"})})]})}function l(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},86006:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/image-20250423182608052-b72dd9e70524d6615c49a00ac320224d.png"},14538:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/image-20250423183009683-979d8516344a5d435f4054bb30e67755.png"},11151:(e,n,i)=>{i.d(n,{Z:()=>d,a:()=>r});var s=i(67294);const t={},_=s.createContext(t);function r(e){const n=s.useContext(_);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(_.Provider,{value:n},e.children)}}}]);